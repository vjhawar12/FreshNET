{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vjhawar12/FreshNET-A-mobileNET-adaptation/blob/main/FreshNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning-bolts"
      ],
      "metadata": {
        "id": "9DCFEjr1bUQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dEyze5_tgqY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBAPWxBiWgPH",
        "outputId": "d7d95ef1-0a67-470d-a997-cea8a4bf0a20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=2hDyy1VYRxMwp31IG3oT9g4QqA3lzh&prompt=consent&token_usage=remote&access_type=offline&code_challenge=9qVHTsGss38khnJHTxlMRpz1Fa5dPabujUwwlp7ZfqE&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: "
          ]
        }
      ],
      "source": [
        "!gcloud auth application-default login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6I26bJ0bd0ck"
      },
      "outputs": [],
      "source": [
        "!gcloud config set project freshnet-466505"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDtB44SnpQ8M"
      },
      "outputs": [],
      "source": [
        "!cd /content/dataset && gcloud storage cp --recursive gs://fruit-images-freshnet ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWsxLrTEvlqB",
        "outputId": "846769aa-c23b-42fa-8632-901fb695757d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/dataset && ls'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!cd /content/dataset && ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dq9Lp9Z9wJEd"
      },
      "outputs": [],
      "source": [
        "path_to_train_imgs = \"\"\n",
        "path_to_test_imgs = \"\"\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "RANDOM_SEED = 42\n",
        "TRAIN_SIZE, VAL_SIZE = 0.8, 0.2\n",
        "IMG_SIZE = 320 # Original images are ~400*400 px so resizing them to 320 retains detail while reducing computational cost\n",
        "EPOCHS = 50\n",
        "WARMUP_DUR = 20 # final epoch number for linear warmup and cosine decay\n",
        "LR_MIN = 0.0001 # minimum learning rate\n",
        "MILD_DROPOUT_RATE = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXmvl6h6stv_"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE, IMG_SIZE),\n",
        "\n",
        "    \"\"\"\n",
        "    Mild preprocessing only. No MixUp, CutMix, RandomCrop, or ColorJitter because a lightweight model like this one\n",
        "    is less likely to overfit. Also, this model needs to perform fine-grained classification,\n",
        "    so aggressive augmentations could distort the small image regions (like signs of fungi or discoloration)\n",
        "    that are crucial for accurate prediction.\n",
        "    \"\"\"\n",
        "\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(IMG_SIZE, IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ_x5765vvpn"
      },
      "outputs": [],
      "source": [
        "full_train_data = ImageFolder(path_to_train_imgs, transform=train_transform)\n",
        "test_data = ImageFolder(path_to_test_imgs, transform=test_transform)\n",
        "train_data, val_data = random_split(full_train_data, [TRAIN_SIZE, VAL_SIZE], generator=RANDOM_SEED)\n",
        "\n",
        "print(f\"{full_train_data.class_to_idx} \\n {train_data.class_to_idx} \\n {test_data.class_to_idx}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_a2E-zQxN6i"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=10)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class InvertedResidual(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, exp_factor, downsample_factor, kernel_size=3):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.exp_factor = exp_factor\n",
        "    self.downsample_factor = downsample_factor\n",
        "    self.kernel_size = kernel_size\n",
        "\n",
        "    out = self.in_channels * self.exp_factor\n",
        "\n",
        "    self.block = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=self.in_channels, out_channels=out, kernel_size=1, stride=1, groups=1), # expansion\n",
        "\n",
        "        \"\"\"\n",
        "        batchnorm normalizes inputs which has shown to improve training. It's typically applied after the convolutional layer output and before the activation function\n",
        "        The MobileNET paper also mentions, with the exception of the final fully connected linear layer, \"all layers are followed by a batchnorm and ReLU nonlinearity\"\n",
        "        \"\"\"\n",
        "        nn.BatchNorm2d(out),\n",
        "\n",
        "        nn.Conv2d(in_channels=out, out_channels=out, kernel_size=self.kernel_size, stride=self.downsample_factor, groups=out), # depthwise\n",
        "        nn.BatchNorm2d(out),\n",
        "        nn.Conv2d(in_channels=out, out_channels=self.out_channels, kernel_size=1, stride=1, groups=1), # pointwise\n",
        "        nn.BatchNorm2d(self.out_channels),\n",
        "\n",
        "        nn.ReLU6(), # output âˆˆ [0, 6] ==> more efficient than regular ReLU\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.block(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "VbsxVpDvSGbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDb7UN4x0Z0z"
      },
      "outputs": [],
      "source": [
        "# A MobileNET adaptation\n",
        "class FreshNET(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  FreshNET applies the concept of depthwise seperable convolutions as mentioned in the MobileNet paper, but the image dimensions are slightly increased\n",
        "  to better suit the dataset and leverage higher image quality. Specifically, FreshNET decouples spatial filtering and channel mixing,\n",
        "  the two operations traditionally combined in standard convolutions and performs them as separate, more efficient operations, thus \"drastically reducing computation\n",
        "  and model size\". Despite this computational efficiency, accuracy is largely preserved -- the original MobileNet paper reports only about a 1% drop in accuracy compared\n",
        "  to standard convolutions.\n",
        "\n",
        "  This separation is implemented in InvertedResidual class. Each instance consists of:\n",
        "  - A pointwise 1*1 convolution for channel expansion, increasing feature dimensionality.\n",
        "  - A depthwise 3*3 convolution applied independently per channel for spatial filtering.\n",
        "  - Another pointwise convolution to project back to a lower-dimensional space.\n",
        "\n",
        "  This design significantly reduces computational cost while maintaining high representational power. There are 10 layers, of which 7 are InvertedResidual layers.\n",
        "  Each InvertedResidual layer consists of 1-4 InvertedResidual instances. Within the InvertedResidual layer, there are non-linear activation functions.\n",
        "\n",
        "  Skip connections are also applied where possible to improve gradient flow and performance on validation data.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # silu is good for vanishing gradients and can be applied in the later layers where ReLU might not be as effective (dead neurons)\n",
        "    silu = nn.SiLU()\n",
        "\n",
        "    # Regularization\n",
        "    \"\"\"\n",
        "    The original paper notes \"we use less regularization and data augmentation techniques because small models have less trouble with overfitting\".\n",
        "    This is why I chose to use dropout with 10% probability since that's on the lower end of the probability spectrum.\n",
        "    \"\"\"\n",
        "    dropout_mild = nn.Dropout(MILD_DROPOUT_RATE)\n",
        "\n",
        "    # initial regular convolution\n",
        "    initial_conv = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2) # 320*320*3 --> 160*160*32\n",
        "\n",
        "    # inverted residual block 1: 160*160*32 --> 160*160*16, channel expansion = 1\n",
        "    block_1 = nn.Sequential(\n",
        "        InvertedResidual(32, 16, 1, 1)\n",
        "    )\n",
        "\n",
        "    # inverted residual block 2: 160*160*16 --> 80*80*24, channel expansion = 6\n",
        "    block_2 = nn.Sequential(\n",
        "        InvertedResidual(16, 24, 6, 2),\n",
        "        InvertedResidual(24, 24, 6, 1)\n",
        "    )\n",
        "\n",
        "    # inverted residual block 3: 80*80*24 --> 40*40*32, channel expansion = 6\n",
        "    block_3 = nn.Sequential(\n",
        "        InvertedResidual(24, 32, 6, 2),\n",
        "        InvertedResidual(32, 32, 6, 1),\n",
        "        InvertedResidual(32, 32, 6, 1),\n",
        "    )\n",
        "\n",
        "    # inverted residual block 4: 40*40*32 --> 20*20*64, channel expansion = 6\n",
        "    block_4 = nn.Sequential(\n",
        "        InvertedResidual(32, 64, 6, 2),\n",
        "        InvertedResidual(64, 64, 6, 1),\n",
        "        InvertedResidual(64, 64, 6, 1),\n",
        "        InvertedResidual(64, 64, 6, 1),\n",
        "    )\n",
        "\n",
        "    # inverted residual block 5: 20*20*64 --> 20*20*96, channel expansion = 6\n",
        "    block_5 = nn.Sequential(\n",
        "        InvertedResidual(64, 96, 6, 1),\n",
        "        InvertedResidual(96, 96, 6, 1),\n",
        "        InvertedResidual(96, 96, 6, 1),\n",
        "    )\n",
        "\n",
        "    # inverted residual block 6: 20*20*96 --> 10*10*160, channel expansion = 6\n",
        "    block_6 = nn.Sequential(\n",
        "        InvertedResidual(96, 160, 6, 2),\n",
        "        InvertedResidual(160, 160, 6, 1),\n",
        "        InvertedResidual(160, 160, 6, 1),\n",
        "    )\n",
        "\n",
        "    # inverted residual block 7: 10*10*160 --> 10*10*320, channel expansion = 6\n",
        "    block_7 = nn.Sequential(\n",
        "        InvertedResidual(160, 320, 6, 1)\n",
        "    )\n",
        "\n",
        "    # final regular convolution\n",
        "    final_conv = nn.Conv2d(in_channels=320, out_channels=1280, kernel_size=1, stride=1) # 10*10*320 --> 10*10*1280\n",
        "\n",
        "    \"\"\"\n",
        "      MaxPooling is often preferred for highlighting dominant features, but it can be too aggressive in lightweight models\n",
        "      like MobileNet or EfficientNet, which already have low spatial resolution. It can discard valuable spatial\n",
        "      information. Average Pooling computes the average of nearby pixels, preserving more context. This makes it more\n",
        "      suitable for compact architectures like this one.\n",
        "    \"\"\"\n",
        "\n",
        "    avg_pool = nn.AvgPool2d(10) # 10*10*1280 --> 1*1*1280\n",
        "\n",
        "    # fully connected layers\n",
        "    \"\"\"\n",
        "    The MobileNET paper mapped from 1280 directly to 1000 since it was being trained on ImageNET. My dataset, however, only has 2\n",
        "    output classes: fresh (0) or rotten (1). Mapping directly from 1280 to 2 is an abrupt jump which could limit the model's ability to\n",
        "    distinguish between classes, so I introduced an additional layer to map from 1280 to 500 then applied nonlinearity and\n",
        "    mild dropout before going from 500 to 2.\n",
        "    \"\"\"\n",
        "    fc_1 = nn.Linear(in_features=1280, out_features=500)\n",
        "    fc_2 = nn.Linear(in_features=500, out_features=2)\n",
        "\n",
        "    # layer order\n",
        "    self.layers = nn.ModuleList(\n",
        "        [\n",
        "            initial_conv,\n",
        "\n",
        "            block_1\n",
        "            block_2,\n",
        "            block_3,\n",
        "            block_4,\n",
        "\n",
        "            dropout_mild,\n",
        "\n",
        "            block_5,\n",
        "            block_6,\n",
        "            block_7,\n",
        "\n",
        "            final_conv,\n",
        "            avg_pool,\n",
        "            silu,\n",
        "\n",
        "            fc_1,\n",
        "            silu,\n",
        "            dropout_mild,\n",
        "\n",
        "            fc_2\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCIQf4NUxhgv"
      },
      "outputs": [],
      "source": [
        "cnn = FreshNET()\n",
        "loss_fn = nn.CrossEntropyLoss() # std loss function for classification\n",
        "\n",
        "# builds upon AdaGRAD but prevents lr from decreasing too much. Used in the original paper.\n",
        "optimizer = optim.RMSProp(cnn.parameters())\n",
        "\n",
        "\"\"\"\n",
        "Not mentioned in the original paper, but this scheduler was used because linear warmup reduces volatility in the earlier epochs.\n",
        "Cosine annealing can improve training stability and convergence and declining LR steadily means the model will rely more on the features\n",
        "it learns early on -- the key differentiators -- rather than picking up potential noise and overfitting.\n",
        "\"\"\"\n",
        "scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=WARMUP_DUR, eta_min=LR_MIN)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_dataloader):\n",
        "  running_loss = 0\n",
        "  accuracy = 0\n",
        "\n",
        "  for batch in train_dataloader:\n",
        "    images, labels = batch\n",
        "    optimizer.zero_grad()\n",
        "    predicted = torch.argmax(model(images))\n",
        "    loss = loss_fn(predicted, labels)\n",
        "    running_loss += loss\n",
        "    accuracy += torch.sum(predicted == labels)\n",
        "\n",
        "    loss.backward()\n",
        "    scheduler.step()\n",
        "\n",
        "  avg_loss = running_loss / len(train_dataloader)\n",
        "  avg_acc = accuracy / len(train_dataloader)\n",
        "\n",
        "  return avg_loss, avg_acc"
      ],
      "metadata": {
        "id": "OL9z2hUExK81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(val_dataloader):\n",
        "  accuracy = 0\n",
        "\n",
        "  for batch in val_dataloader:\n",
        "    images, labels = batch\n",
        "    predicted = torch.argmax(model(images))\n",
        "    accuracy += torch.sum(predicted == labels)\n",
        "\n",
        "  avg_acc = accuracy / len(val_dataloader)\n",
        "\n",
        "  return avg_acc"
      ],
      "metadata": {
        "id": "561tlOBf4GOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(test_dataloader):\n",
        "  accuracy = 0\n",
        "\n",
        "  for batch in test_dataloader:\n",
        "    images, labels = batch\n",
        "    predicted = torch.argmax(model(images))\n",
        "    accuracy += torch.sum(predicted == labels)\n",
        "\n",
        "  avg_acc = accuracy / len(test_dataloader)\n",
        "\n",
        "  return avg_acc"
      ],
      "metadata": {
        "id": "Tk4rp3ox4I7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loop = tqdm(range(EPOCHS)) # progress bar\n",
        "\n",
        "for epoch in loop:\n",
        "  cnn.train()\n",
        "  train_loss, train_acc = train(train_dataloader)\n",
        "\n",
        "  cnn.eval()\n",
        "  with torch.no_grad():\n",
        "    val_acc = validate(val_dataloader)\n",
        "\n",
        "  loop.set_description(f\"Train Loss: {train_loss} \\t Train acc: {100 * train_acc}% \\t Val acc: {100 * val_acc}%\")"
      ],
      "metadata": {
        "id": "yFiUCT5I6g64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.eval()\n",
        "train_acc = test(test_dataloader)\n",
        "print(f\"Final test accuracy: {100 * train_acc}%\")"
      ],
      "metadata": {
        "id": "KY-5zlw17L-I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGInfmOLmslzKhWJIiDFXT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}