{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vjhawar12/FreshNET-A-mobileNET-adaptation/blob/main/FreshNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "VD5TbE7WQKdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install lightning-bolts"
      ],
      "metadata": {
        "id": "9DCFEjr1bUQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dEyze5_tgqY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
        "from copy import deepcopy\n",
        "from torch.quantization import QuantStub, DeQuantStub, prepare_qat, get_default_qat_qconfig, convert, fuse_modules"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading dataset from Google Cloud Storage"
      ],
      "metadata": {
        "id": "PLDhrdLkQOgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth login"
      ],
      "metadata": {
        "id": "Z6kjxCQBG33V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "id": "xpxNoZaCCgZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6I26bJ0bd0ck"
      },
      "outputs": [],
      "source": [
        "!gcloud config set project freshnet-466505"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_bucket_with_transfer_manager(bucket_name, destination_directory=\"\", workers=8, max_results=1000):\n",
        "\n",
        "    from google.cloud.storage import Client, transfer_manager\n",
        "\n",
        "    client = Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "\n",
        "    blob_names = [blob.name for blob in bucket.list_blobs(max_results=max_results)]\n",
        "\n",
        "    results = transfer_manager.download_many_to_path(\n",
        "        bucket, blob_names, destination_directory=destination_directory, max_workers=workers\n",
        "    )\n",
        "\n",
        "    for name, result in zip(blob_names, results):\n",
        "        if isinstance(result, Exception):\n",
        "            print(\"Failed to download {} due to exception: {}\".format(name, result))\n",
        "        else:\n",
        "            print(\"Downloaded {} to {}.\".format(name, destination_directory + name))\n",
        "\n",
        "download_bucket_with_transfer_manager(\"freshnet-images\", destination_directory=\"/content/dataset/\", workers=8, max_results=None)"
      ],
      "metadata": {
        "id": "gVKqs6cjAvE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/dataset/train/rotten && unzip -j train_rotten_images.zip"
      ],
      "metadata": {
        "id": "1ePoZVvuDYGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/dataset/train/fresh && unzip -j train_fresh_images.zip"
      ],
      "metadata": {
        "id": "BNDkmH8NDxwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/dataset/test/fresh && unzip -j test_fresh_images.zip"
      ],
      "metadata": {
        "id": "AkCehLpLD_Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/dataset/test/rotten && unzip -j test_rotten_images.zip"
      ],
      "metadata": {
        "id": "gOoxJ2B4ECtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "wseUCgG6QUXn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dq9Lp9Z9wJEd"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 256\n",
        "RANDOM_SEED = 42\n",
        "TRAIN_SIZE, VAL_SIZE = 0.8, 0.2\n",
        "IMG_SIZE = 320 # Original images are ~400*400 px so resizing them to 320 retains detail while reducing computational cost\n",
        "EPOCHS = 30\n",
        "WARMUP_DUR = 10 # num of warmup epochs\n",
        "LR_MIN = 0.0001 # minimum learning rate\n",
        "MILD_DROPOUT_RATE = 0.10 # mild enough to avoid overfitting on a small dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "Ol81UPOCQc6P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXmvl6h6stv_"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Mild preprocessing only. No MixUp, CutMix, RandomCrop, or ColorJitter because a lightweight model like this one\n",
        "    is less likely to overfit. Also, this model needs to perform fine-grained classification,\n",
        "    so aggressive augmentations could distort the small image regions (like signs of fungi or discoloration)\n",
        "    that are crucial for accurate prediction.\n",
        "\"\"\"\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation([-180, 180]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data into Dataloader"
      ],
      "metadata": {
        "id": "WFRogiAGQhks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQ_x5765vvpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60a31aac-f7fe-4161-e0a4-db142ae78dc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fresh': 0, 'rotten': 1} \n",
            " {'fresh': 0, 'rotten': 1}\n"
          ]
        }
      ],
      "source": [
        "path_to_train_imgs = \"/content/dataset/train\"\n",
        "path_to_test_imgs = \"/content/dataset/test\"\n",
        "\n",
        "full_train_data = ImageFolder(path_to_train_imgs, transform=train_transform)\n",
        "test_data = ImageFolder(path_to_test_imgs, transform=test_transform)\n",
        "train_data, val_data = random_split(full_train_data, [TRAIN_SIZE, VAL_SIZE], generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
        "\n",
        "print(f\"{full_train_data.class_to_idx} \\n {test_data.class_to_idx}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_a2E-zQxN6i"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=10)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "indices_to_keep = [i for i, (image, label) in enumerate(test_data) if i < 20]\n",
        "mini = Subset(test_data, indices_to_keep)\n",
        "mini_test_dataloader = DataLoader(mini, batch_size=5, shuffle=False, num_workers=4)\n",
        "\n",
        "indices_to_keep = [i for i, (image, label) in enumerate(test_data) if i > 20 and i < 40]\n",
        "calibrate = Subset(test_data, indices_to_keep)\n",
        "calibrate_dataloader = DataLoader(calibrate, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA optimizations"
      ],
      "metadata": {
        "id": "k_hvEYYCQlGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.backends.cuda.matmul.allow_tf32 = True\n",
        "  torch.backends.cudnn.allow_tf32 = True\n",
        "  torch.backends.cuda.enable_flash_sdp(True)\n",
        "  torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "  torch.backends.cuda.enable_math_sdp(True)"
      ],
      "metadata": {
        "id": "Q74Vsb5yaKpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization helper class: Float Wrapper\n",
        "\n",
        "This class wraps operations that Pytorch cannot quantize, such as deptwise convolutions with groups != 1, with a DeQuantStub and a QuantStub. This ensures during quantization the particular operations wrapped by a FloatWrapper instance receive float32 objects and not INT8."
      ],
      "metadata": {
        "id": "GPSUbTvGRN3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FloatWrapper(nn.Module):\n",
        "    def __init__(self, layer):\n",
        "        super().__init__()\n",
        "        self.layer = layer\n",
        "        self.quant = QuantStub()\n",
        "        self.dequant = DeQuantStub()\n",
        "        self.qconfig = None\n",
        "        self.layer.qconfig = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dequant(x)\n",
        "        x = self.layer(x)\n",
        "        x = self.quant(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "R--PMV9TRsgw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Depthwise Seperable Convolution implementation"
      ],
      "metadata": {
        "id": "jmAOovOeQoIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DepthwiseSeperableConvolution(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, exp_factor, downsample_factor, kernel_size=3):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.exp_factor = exp_factor\n",
        "    self.downsample_factor = downsample_factor\n",
        "    self.kernel_size = kernel_size\n",
        "\n",
        "    out = self.in_channels * self.exp_factor\n",
        "\n",
        "    self.preserve = self.in_channels == self.out_channels\n",
        "    \"\"\"\n",
        "        batchnorm normalizes inputs which has shown to improve training. It's typically applied after the convolutional layer output and before the activation function\n",
        "        The MobileNET paper also mentions, with the exception of the final fully connected linear layer, \"all layers are followed by a batchnorm and ReLU nonlinearity\"\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    self.block = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=self.in_channels, out_channels=out, kernel_size=1, stride=1, groups=1), # expansion\n",
        "        nn.BatchNorm2d(out),\n",
        "\n",
        "        FloatWrapper(\n",
        "            # adding padding because the image downsamples, but skip connections require same size\n",
        "            nn.Conv2d(in_channels=out, out_channels=out, kernel_size=self.kernel_size, padding=self.kernel_size // 2, stride=self.downsample_factor, groups=out)\n",
        "            # depthwise\n",
        "        ),\n",
        "\n",
        "        nn.BatchNorm2d(out),\n",
        "        nn.Conv2d(in_channels=out, out_channels=self.out_channels, kernel_size=1, stride=1, groups=1), # pointwise\n",
        "        nn.BatchNorm2d(self.out_channels),\n",
        "\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.block:\n",
        "      x = layer(x)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "VbsxVpDvSGbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skip connection"
      ],
      "metadata": {
        "id": "N845-3DoQrJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "FreshNET only applies skip connections between DepthwiseSeperableConvolution instances which preserve channel dimension in order to adhere to the laws of vector addition.\n",
        "\"\"\"\n",
        "class SkipConnection(nn.Module):\n",
        "  def __init__(self, seq): # seq is an nn.Sequential instance\n",
        "    super().__init__()\n",
        "    self.quant = QuantStub()\n",
        "    self.dequant = DeQuantStub()\n",
        "    self.seq = seq\n",
        "\n",
        "    for module in seq:\n",
        "      if not hasattr(module, 'preserve'):\n",
        "        raise Exception(\"Cannot apply skip connection between layers of different channel dimensions\")\n",
        "\n",
        "    self.seq = seq\n",
        "\n",
        "  def forward(self, x):\n",
        "    # addition is not supported with quantized tensors so we dequant, perform addition, then quant again\n",
        "    x = self.dequant(x)\n",
        "    self.gradient = x\n",
        "\n",
        "    for depthwise_sep_conv in self.seq:\n",
        "      x = depthwise_sep_conv(x)\n",
        "\n",
        "    x = self.quant(x + self.gradient)\n",
        "    return x"
      ],
      "metadata": {
        "id": "z3wLK60ji3mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FreshNet: A MobileNET adaptation"
      ],
      "metadata": {
        "id": "5dYcj9FeQt25"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDb7UN4x0Z0z"
      },
      "outputs": [],
      "source": [
        "# A MobileNET adaptation\n",
        "class FreshNET(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  FreshNET applies the concept of depthwise seperable convolutions as mentioned in the MobileNet paper, but the image dimensions are slightly increased\n",
        "  to better suit the dataset and leverage higher image quality. Specifically, FreshNET decouples spatial filtering and channel mixing,\n",
        "  the two operations traditionally combined in standard convolutions, and performs them as separate, more efficient operations. As the authors of MobileNet noted,\n",
        "  it \"drastically reduc[es] computation and model size\". Despite this computational efficiency, accuracy is largely preserved -- the original MobileNet paper reports\n",
        "  only about a 1% drop in accuracy compared to standard convolutions.\n",
        "\n",
        "  This separation is implemented in DepthwiseSeperableConvolution class. Each instance consists of:\n",
        "  - A pointwise 1*1 convolution for channel expansion, increasing feature dimensionality.\n",
        "  - A depthwise 3*3 convolution applied independently per channel for spatial filtering.\n",
        "  - Another pointwise convolution to project back to a lower-dimensional space.\n",
        "\n",
        "  This design significantly reduces computational cost while maintaining high representational power. There are 10 layers, of which 7 are DepthwiseSeperableConvolution layers.\n",
        "  Each DepthwiseSeperableConvolution layer consists of 1-4 DepthwiseSeperableConvolution instances. Within the DepthwiseSeperableConvolution layer, there are non-linear activation functions.\n",
        "\n",
        "  Skip connections are also applied where possible to improve gradient flow and preserve fine-grained information. For a task like fresh/rotten classification,\n",
        "  fine-grained information can be very valuable.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # QuantStub and DeQuantStub are used to go from FL32 <===> quantized\n",
        "    self.quant = QuantStub()\n",
        "    self.dequant = DeQuantStub()\n",
        "\n",
        "    # SiLU is good for vanishing gradients and can be applied in the later layers where ReLU might not be as effective (dead neurons)\n",
        "    silu = nn.SiLU()\n",
        "\n",
        "    # Regularization\n",
        "    \"\"\"\n",
        "    The original paper notes \"we use less regularization and data augmentation techniques because small models have less trouble with overfitting\".\n",
        "    This is why I chose to use dropout with 15% probability since that's on the lower end of the probability spectrum.\n",
        "    \"\"\"\n",
        "    dropout_mild = nn.Dropout(MILD_DROPOUT_RATE)\n",
        "\n",
        "    # initial regular convolution\n",
        "    block_0 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2),\n",
        "        nn.BatchNorm2d(),\n",
        "        nn.ReLU(),\n",
        "\n",
        "    ) # 320*320*3 --> 160*160*32\n",
        "\n",
        "    # inverted residual block 1: 160*160*32 --> 160*160*16, channel expansion = 1\n",
        "    block_1 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(32, 16, 1, 1)\n",
        "    )\n",
        "\n",
        "    # inverted residual block 2: 160*160*16 --> 80*80*24, channel expansion = 6\n",
        "    block_2 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(16, 24, 6, 2),\n",
        "        DepthwiseSeperableConvolution(24, 24, 6, 1)\n",
        "    )\n",
        "\n",
        "    # inverted residual block 3: 80*80*24 --> 40*40*32, channel expansion = 6\n",
        "    block_3 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(24, 32, 6, 2),\n",
        "        SkipConnection(nn.Sequential(\n",
        "              DepthwiseSeperableConvolution(32, 32, 6, 1),\n",
        "              DepthwiseSeperableConvolution(32, 32, 6, 1),\n",
        "          )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # inverted residual block 4: 40*40*32 --> 20*20*64, channel expansion = 6\n",
        "    block_4 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(32, 64, 6, 2),\n",
        "        SkipConnection(nn.Sequential(\n",
        "              DepthwiseSeperableConvolution(64, 64, 6, 1),\n",
        "              DepthwiseSeperableConvolution(64, 64, 6, 1),\n",
        "              DepthwiseSeperableConvolution(64, 64, 6, 1),\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # inverted residual block 5: 20*20*64 --> 20*20*96, channel expansion = 6\n",
        "    block_5 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(64, 96, 6, 1),\n",
        "        SkipConnection(nn.Sequential(\n",
        "              DepthwiseSeperableConvolution(96, 96, 6, 1),\n",
        "              DepthwiseSeperableConvolution(96, 96, 6, 1),\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # inverted residual block 6: 20*20*96 --> 10*10*160, channel expansion = 6\n",
        "    block_6 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(96, 160, 6, 2),\n",
        "        SkipConnection(nn.Sequential(\n",
        "              DepthwiseSeperableConvolution(160, 160, 6, 1),\n",
        "              DepthwiseSeperableConvolution(160, 160, 6, 1),\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # inverted residual block 7: 10*10*160 --> 10*10*320, channel expansion = 6\n",
        "    block_7 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(160, 320, 6, 1)\n",
        "    )\n",
        "\n",
        "    # final regular convolution\n",
        "    final_conv = nn.Conv2d(in_channels=320, out_channels=1280, kernel_size=1, stride=1) # 10*10*320 --> 10*10*1280\n",
        "\n",
        "    \"\"\"\n",
        "      MaxPooling is often preferred for highlighting dominant features, but it can be too aggressive in lightweight models\n",
        "      like MobileNet or EfficientNet, which already have low spatial resolution. It can discard valuable spatial\n",
        "      information. Average Pooling computes the average of nearby pixels, preserving more context. This makes it more\n",
        "      suitable for compact architectures like this one.\n",
        "    \"\"\"\n",
        "\n",
        "    avg_pool = nn.AvgPool2d(10) # 10*10*1280 --> 1*1*1280\n",
        "    flatten = nn.Flatten() # fc layers expect a single tensor not a feature map\n",
        "\n",
        "    # fully connected layers\n",
        "    \"\"\"\n",
        "    The MobileNET paper mapped from 1280 directly to 1000 since it was being trained on ImageNET. My dataset, however, only has 2\n",
        "    output classes: fresh (0) or rotten (1). Mapping directly from 1280 to 2 is an abrupt jump which could limit the model's ability to\n",
        "    distinguish between classes, so I introduced an additional layer to map from 1280 to 500 then applied nonlinearity and\n",
        "    mild dropout before going from 500 to 2.\n",
        "    \"\"\"\n",
        "    fc_1 = nn.Linear(in_features=1280, out_features=500)\n",
        "    fc_2 = nn.Linear(in_features=500, out_features=2)\n",
        "\n",
        "    \"\"\"\n",
        "    The sequence of layers is as follows:\n",
        "\n",
        "      - intial regular convolution\n",
        "      - 4 stacks of DepthwiseSeperableConvolution\n",
        "      - mild dropout for regularization\n",
        "      - 3 more stacks of DepthwiseSeperableConvolution\n",
        "      - 1 final regular convolution\n",
        "      - average pooling\n",
        "      - activation function\n",
        "      - first fully connected linear layer\n",
        "      - activation function\n",
        "      - mild dropout for regularization\n",
        "      - final fully connected linear layer\n",
        "\n",
        "    There are skip connections in blocks 3-6.\n",
        "    \"\"\"\n",
        "\n",
        "    self.layers = nn.Sequential(\n",
        "        block_0,\n",
        "\n",
        "        block_1,\n",
        "        block_2,\n",
        "        block_3,\n",
        "        block_4,\n",
        "        FloatWrapper(dropout_mild),\n",
        "\n",
        "        block_5,\n",
        "        block_6,\n",
        "        block_7,\n",
        "\n",
        "        final_conv,\n",
        "        avg_pool,\n",
        "        flatten,\n",
        "        FloatWrapper(silu),\n",
        "        FloatWrapper(dropout_mild),\n",
        "\n",
        "        fc_1,\n",
        "        FloatWrapper(silu),\n",
        "\n",
        "        fc_2\n",
        "    )\n",
        "\n",
        "  # fusing conv + bn + relu improves speed and efficiency during QAT\n",
        "  def fuse(self):\n",
        "    for module_name, module in self.named_modules(): # iterating over the instances defined in __init__ of this class\n",
        "      if isinstance(module, DepthwiseSeperableConvolution):\n",
        "        if isinstance(module, nn.Sequential):\n",
        "          fuse_modules(module, [[\"0\", \"1\"], [\"2\", \"3\"], [\"4\", \"5\", \"6\"]], inplace=True)\n",
        "      elif isinstance(module, nn.Sequential):\n",
        "        fuse_modules(module, [[\"0\", \"1\", \"2\"]], inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.quant(x)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    x = self.dequant(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "GIaS73iFUUBE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCIQf4NUxhgv"
      },
      "outputs": [],
      "source": [
        "cnn = FreshNET()\n",
        "cnn.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss() # std loss function for classification\n",
        "\n",
        "optimizer = optim.Adam(cnn.parameters()) # RMSProp is pretty sensitive to changes in LR so i wanted to try Adam\n",
        "\n",
        "scaler = torch.amp.GradScaler()\n",
        "\n",
        "\"\"\"\n",
        "Not mentioned in the original paper, but this scheduler was used because linear warmup reduces volatility in the earlier epochs.\n",
        "Cosine annealing can improve training stability and convergence and declining LR steadily means the model will rely more on the features\n",
        "it learns early on -- the key differentiators -- rather than picking up potential noise and overfitting.\n",
        "\"\"\"\n",
        "scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=WARMUP_DUR, eta_min=LR_MIN, max_epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and validate"
      ],
      "metadata": {
        "id": "afXvV6nhQynV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(cnn, epochs=EPOCHS, scaler=None):\n",
        "  cnn.train()\n",
        "\n",
        "  for i in range(epochs):\n",
        "    torch.cuda.empty_cache()\n",
        "    correct, total = 0, 0\n",
        "    running_loss = 0\n",
        "\n",
        "    cnn.train(True)\n",
        "\n",
        "    loop = tqdm(train_dataloader, desc=f\"Epoch {i+1}/{epochs}\", leave=True, disable=False)\n",
        "\n",
        "    for j, (input, labels) in enumerate(loop, 1):\n",
        "      input, labels = input.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = cnn(input)\n",
        "      loss = loss_fn(output, labels)\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      if scaler:\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "      else:\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      avg_loss = running_loss / j\n",
        "      loop.set_postfix(loss=avg_loss)\n",
        "\n",
        "      pred = torch.argmax(output, dim=1)\n",
        "\n",
        "      total += labels.size(0)\n",
        "      correct += (pred == labels).sum().item()\n",
        "\n",
        "\n",
        "    cnn.eval()\n",
        "    val_total, val_correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for image, label in val_dataloader:\n",
        "        image, label = image.to(device), label.to(device)\n",
        "        output = cnn(image)\n",
        "\n",
        "        pred = torch.argmax(output, dim=1)\n",
        "        val_total += label.size(0)\n",
        "        val_correct += (pred == label).sum().item()\n",
        "\n",
        "      val_accuracy = val_correct / val_total\n",
        "      accuracy = correct / total\n",
        "\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Epoch {i + 1}: LR={current_lr:.6f} \\t Train Acc: {accuracy:.4f} \\t Val Acc: {val_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "pMzwv9TEVZCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.to(\"cuda\")\n",
        "compiled_cnn = torch.compile(cnn)\n",
        "\n",
        "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "  train(compiled_cnn, scaler=scaler)"
      ],
      "metadata": {
        "id": "yFiUCT5I6g64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c72c557-f8dd-47c3-a5e3-018d54b7e3cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/30: 100%|██████████| 74/74 [03:36<00:00,  2.92s/it, loss=0.692]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: LR=0.000111 \t Train Acc: 0.5250 \t Val Acc: 0.5289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/30: 100%|██████████| 74/74 [00:28<00:00,  2.60it/s, loss=0.416]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: LR=0.000222 \t Train Acc: 0.8011 \t Val Acc: 0.8562\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/30: 100%|██████████| 74/74 [00:28<00:00,  2.61it/s, loss=0.286]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: LR=0.000333 \t Train Acc: 0.8826 \t Val Acc: 0.8850\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/30: 100%|██████████| 74/74 [00:28<00:00,  2.60it/s, loss=0.252]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: LR=0.000444 \t Train Acc: 0.8984 \t Val Acc: 0.9071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/30: 100%|██████████| 74/74 [00:28<00:00,  2.63it/s, loss=0.221]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: LR=0.000556 \t Train Acc: 0.9123 \t Val Acc: 0.9011\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/30: 100%|██████████| 74/74 [00:28<00:00,  2.60it/s, loss=0.204]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: LR=0.000667 \t Train Acc: 0.9170 \t Val Acc: 0.8829\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/30: 100%|██████████| 74/74 [00:28<00:00,  2.59it/s, loss=0.192]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: LR=0.000778 \t Train Acc: 0.9227 \t Val Acc: 0.9138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/30: 100%|██████████| 74/74 [00:28<00:00,  2.57it/s, loss=0.17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: LR=0.000889 \t Train Acc: 0.9307 \t Val Acc: 0.8833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/30: 100%|██████████| 74/74 [00:28<00:00,  2.61it/s, loss=0.167]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: LR=0.001000 \t Train Acc: 0.9320 \t Val Acc: 0.9405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/30: 100%|██████████| 74/74 [00:28<00:00,  2.60it/s, loss=0.148]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: LR=0.001000 \t Train Acc: 0.9393 \t Val Acc: 0.9115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/30: 100%|██████████| 74/74 [00:28<00:00,  2.60it/s, loss=0.139]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: LR=0.000994 \t Train Acc: 0.9432 \t Val Acc: 0.9519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/30: 100%|██████████| 74/74 [00:28<00:00,  2.58it/s, loss=0.114]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: LR=0.000978 \t Train Acc: 0.9532 \t Val Acc: 0.9439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/30: 100%|██████████| 74/74 [00:28<00:00,  2.63it/s, loss=0.116]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: LR=0.000951 \t Train Acc: 0.9525 \t Val Acc: 0.9280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/30: 100%|██████████| 74/74 [00:28<00:00,  2.60it/s, loss=0.0962]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: LR=0.000914 \t Train Acc: 0.9616 \t Val Acc: 0.9608\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/30: 100%|██████████| 74/74 [00:28<00:00,  2.59it/s, loss=0.0862]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: LR=0.000868 \t Train Acc: 0.9654 \t Val Acc: 0.9593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/30: 100%|██████████| 74/74 [00:28<00:00,  2.60it/s, loss=0.0845]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: LR=0.000815 \t Train Acc: 0.9675 \t Val Acc: 0.9638\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/30: 100%|██████████| 74/74 [00:28<00:00,  2.59it/s, loss=0.0832]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17: LR=0.000754 \t Train Acc: 0.9679 \t Val Acc: 0.9564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/30: 100%|██████████| 74/74 [00:28<00:00,  2.59it/s, loss=0.0803]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18: LR=0.000689 \t Train Acc: 0.9684 \t Val Acc: 0.9723\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/30: 100%|██████████| 74/74 [00:28<00:00,  2.61it/s, loss=0.071]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19: LR=0.000620 \t Train Acc: 0.9722 \t Val Acc: 0.9308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/30: 100%|██████████| 74/74 [00:28<00:00,  2.57it/s, loss=0.0579]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20: LR=0.000550 \t Train Acc: 0.9771 \t Val Acc: 0.9763\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/30: 100%|██████████| 74/74 [00:28<00:00,  2.60it/s, loss=0.0619]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21: LR=0.000480 \t Train Acc: 0.9767 \t Val Acc: 0.9729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/30: 100%|██████████| 74/74 [00:28<00:00,  2.59it/s, loss=0.0538]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22: LR=0.000411 \t Train Acc: 0.9789 \t Val Acc: 0.9754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/30: 100%|██████████| 74/74 [00:28<00:00,  2.61it/s, loss=0.0457]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23: LR=0.000346 \t Train Acc: 0.9827 \t Val Acc: 0.9824\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/30: 100%|██████████| 74/74 [00:27<00:00,  2.66it/s, loss=0.0446]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24: LR=0.000285 \t Train Acc: 0.9827 \t Val Acc: 0.9828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/30: 100%|██████████| 74/74 [00:28<00:00,  2.61it/s, loss=0.043]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25: LR=0.000232 \t Train Acc: 0.9833 \t Val Acc: 0.9822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/30: 100%|██████████| 74/74 [00:28<00:00,  2.64it/s, loss=0.0379]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26: LR=0.000186 \t Train Acc: 0.9855 \t Val Acc: 0.9814\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/30: 100%|██████████| 74/74 [00:28<00:00,  2.59it/s, loss=0.0332]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27: LR=0.000149 \t Train Acc: 0.9874 \t Val Acc: 0.9860\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/30: 100%|██████████| 74/74 [00:28<00:00,  2.59it/s, loss=0.0334]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28: LR=0.000122 \t Train Acc: 0.9873 \t Val Acc: 0.9903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/30: 100%|██████████| 74/74 [00:28<00:00,  2.61it/s, loss=0.032]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29: LR=0.000106 \t Train Acc: 0.9883 \t Val Acc: 0.9877\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/30: 100%|██████████| 74/74 [00:28<00:00,  2.59it/s, loss=0.0289]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30: LR=0.000100 \t Train Acc: 0.9891 \t Val Acc: 0.9875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving model weights"
      ],
      "metadata": {
        "id": "k--Nu6t55uLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.to(\"cpu\")\n",
        "torch.save(cnn.state_dict(), \"/content/modelweight.pth\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(r\"/content/modelweight.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Ex61Q4tC5wU6",
        "outputId": "96681c45-86ce-4f43-fd7c-7fefcdff91c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_790e6a81-0d03-4190-a2d0-fac7282be665\", \"modelweight.pth\", 11774172)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading saved model"
      ],
      "metadata": {
        "id": "iAJdALBLE1Rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "#files.upload()\n",
        "weights = torch.load('/content/modelweight.pth', map_location=torch.device('cpu'))\n",
        "cnn = FreshNET()\n",
        "cnn.load_state_dict(weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tfHgo4CE2fC",
        "outputId": "78716353-bae3-4013-ef39-f77b5aec3484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model size (no quantization)"
      ],
      "metadata": {
        "id": "Gc5_kJ5ebGJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_size = 0\n",
        "\n",
        "for param in cnn.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "\n",
        "buffer_size = 0\n",
        "\n",
        "for buffer in cnn.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('model size: {:.3f}MB'.format(size_all_mb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3Ly9vrv0MVo",
        "outputId": "767b271c-b36e-45ea-f9f4-a776215c91b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size: 11.111MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final test accuracy (no quantization)"
      ],
      "metadata": {
        "id": "WZUAMYaCQ7rT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(test_dataloader, cnn):\n",
        "  with torch.no_grad():\n",
        "    cnn.eval()\n",
        "    accuracy = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "      images, labels = batch\n",
        "\n",
        "      images = images.to(\"cpu\")\n",
        "      labels = labels.to(\"cpu\")\n",
        "\n",
        "      if images.dtype != torch.float32:\n",
        "        raise Exception(\"Pre-Quantization invalid data type\")\n",
        "\n",
        "      output = cnn(images)\n",
        "\n",
        "      if images.dtype != torch.float32:\n",
        "        raise Exception(\"Post-Quantization invalid data type\")\n",
        "\n",
        "      predicted = torch.argmax(output, dim=1)\n",
        "      accuracy += (predicted == labels).sum().item()\n",
        "      total += labels.size(0)\n",
        "\n",
        "    avg_acc = accuracy / total\n",
        "\n",
        "    return avg_acc\n",
        "\n",
        "\n",
        "cnn.eval()\n",
        "\n",
        "test_acc = test(test_dataloader, cnn)\n",
        "print(f\"Final test accuracy: {100 * test_acc:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Tk4rp3ox4I7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.to(\"cuda\")\n",
        "cnn.eval()\n",
        "\n",
        "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "  test_acc = test(test_dataloader, cnn)\n",
        "  print(f\"Final test accuracy: {100 * test_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRmnJQDB0LF8",
        "outputId": "5bffb2a5-984e-4a73-eef6-109e9e814d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final test accuracy: 97.61%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantizing model\n",
        "\n",
        "Quantization Aware Training (QAT) involves training the model with full floating point precision, float32, while simulating the effects of int8 quantization. This 'fake' quantization makes the model well-equipped to perform under quantization, maintaining accuracy better than if it used post-training  quantization instead.\n"
      ],
      "metadata": {
        "id": "8oDdlacxT0dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.to(\"cpu\") # model must be on cpu while fusing\n",
        "cnn.train()\n",
        "cnn.fuse() # custom fuse method fuses the DepthwiseSeperableConvolution layers like conv2d, batchnorm, and relu\n",
        "cnn.to(\"cpu\") # need to switch back to cpu before quantizing\n",
        "config = get_default_qat_qconfig(\"qnnpack\") # for ARM-based architecture, qnnpack is the correct config mode\n",
        "cnn.qconfig = config\n",
        "prepare_qat(cnn, inplace=True) # preparing the model for QAT\n",
        "cnn.eval()"
      ],
      "metadata": {
        "id": "Ad8FNPpRw5wS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calibrating model before converting to get meaningful zero_point and scale values\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in calibrate_dataloader:\n",
        "        images, _ = batch\n",
        "        images = images.to(\"cpu\")\n",
        "        output = cnn(images)\n",
        "\n",
        "\n",
        "for name, module in cnn.named_modules():\n",
        "    if hasattr(module, \"activation_post_process\"):\n",
        "        obs = module.activation_post_process\n",
        "        if hasattr(obs, \"calculate_qparams\"):\n",
        "            scale, zero_point = obs.calculate_qparams()\n",
        "            print(f\"Using observer from: {name}\")\n",
        "            break\n",
        "\n",
        "scale = scale.item()\n",
        "zero_point = zero_point.item()\n",
        "\n",
        "print(scale)\n",
        "print(zero_point)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcME8HuDw9iN",
        "outputId": "c562ed47-9f4c-4a66-b3b3-9f10c8e1f078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/utils.py:408: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_cnn = convert(cnn, inplace=False)\n",
        "quantized_cnn.to(\"cpu\")"
      ],
      "metadata": {
        "id": "ng4a2FNZSP7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in quantized_cnn.named_modules():\n",
        "    if isinstance(module, torch.nn.quantized.Conv2d):\n",
        "        print(f\"{name} bias dtype: {module.bias().dtype}\")"
      ],
      "metadata": {
        "id": "NZkgE72dTNTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model size (with quantization)\n"
      ],
      "metadata": {
        "id": "1A0yeJyQbL6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_size = 0\n",
        "\n",
        "for param in quantized_cnn.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "\n",
        "buffer_size = 0\n",
        "\n",
        "for buffer in quantized_cnn.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('model size: {:.3f}MB'.format(size_all_mb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQOpdSI-0Jrl",
        "outputId": "cfa05e02-d4f9-4b53-e51e-339a3d81f54c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model size: 0.242MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final test accuracy (with quantization)"
      ],
      "metadata": {
        "id": "4eaqc1MPan3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_cnn.eval()\n",
        "quantized_cnn.to(\"cuda\")\n",
        "test_acc = test(test_dataloader, quantized_cnn)\n",
        "print(f\"Final test accuracy of quantized model: {100 * test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "eXnvHqmsnBmB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyP1dlummXJpHgk1V4hUVh3u",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}