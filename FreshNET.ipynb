{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vjhawar12/FreshNET-A-mobileNET-adaptation/blob/main/FreshNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "VD5TbE7WQKdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torch==2.6.0 && pip -q install torchvision==0.21.0 && pip -q install torchaudio==2.6.0 && pip -q install lightning-bolts && pip -q install torchao && pip -q install executorch"
      ],
      "metadata": {
        "id": "9DCFEjr1bUQQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c667d350-d3c9-4752-88ec-f56fe3092456"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.22.0 requires torch==2.7.0, but you have torch 2.6.0 which is incompatible.\n",
            "torchaudio 2.7.0 requires torch==2.7.0, but you have torch 2.6.0 which is incompatible.\n",
            "executorch 0.6.0 requires torch==2.7.0, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2dEyze5_tgqY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "8ef5c87c-147e-452a-9ad6-cb677d134454"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'prepare_qat_pt2e' from 'torch.ao.quantization.pt2e' (/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/pt2e/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-2902567769.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpl_bolts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearWarmupCosineAnnealingLR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_export\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcapture_pre_autograd_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m from torch.ao.quantization.pt2e import (\n\u001b[0m\u001b[1;32m     11\u001b[0m      \u001b[0mprepare_qat_pt2e\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mconvert_pt2e\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'prepare_qat_pt2e' from 'torch.ao.quantization.pt2e' (/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/pt2e/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from tqdm import tqdm\n",
        "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
        "from torch._export import capture_pre_autograd_graph\n",
        "from torch.ao.quantization.pt2e import (\n",
        "     prepare_qat_pt2e,\n",
        "    convert_pt2e,\n",
        ")\n",
        "from executorch.backends.xnnpack.quantizer.xnnpack_quantizer import (\n",
        "  get_symmetric_quantization_config,\n",
        "  XNNPACKQuantizer,\n",
        ")\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading dataset from Google Cloud Storage"
      ],
      "metadata": {
        "id": "PLDhrdLkQOgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth login"
      ],
      "metadata": {
        "id": "Z6kjxCQBG33V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "id": "xpxNoZaCCgZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6I26bJ0bd0ck"
      },
      "outputs": [],
      "source": [
        "!gcloud config set project freshnet-466505"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_bucket_with_transfer_manager(bucket_name, destination_directory=\"\", workers=8, max_results=1000):\n",
        "\n",
        "    from google.cloud.storage import Client, transfer_manager\n",
        "\n",
        "    client = Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "\n",
        "    blob_names = [blob.name for blob in bucket.list_blobs(max_results=max_results)]\n",
        "\n",
        "    results = transfer_manager.download_many_to_path(\n",
        "        bucket, blob_names, destination_directory=destination_directory, max_workers=workers\n",
        "    )\n",
        "\n",
        "    for name, result in zip(blob_names, results):\n",
        "        if isinstance(result, Exception):\n",
        "            print(\"Failed to download {} due to exception: {}\".format(name, result))\n",
        "        else:\n",
        "            print(\"Downloaded {} to {}.\".format(name, destination_directory + name))\n",
        "\n",
        "download_bucket_with_transfer_manager(\"freshnet-images\", destination_directory=\"/content/dataset/\", workers=8, max_results=None)"
      ],
      "metadata": {
        "id": "gVKqs6cjAvE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/dataset/train/rotten && unzip -j train_rotten_images.zip"
      ],
      "metadata": {
        "id": "1ePoZVvuDYGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/dataset/train/fresh && unzip -j train_fresh_images.zip"
      ],
      "metadata": {
        "id": "BNDkmH8NDxwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/dataset/test/fresh && unzip -j test_fresh_images.zip"
      ],
      "metadata": {
        "id": "AkCehLpLD_Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/dataset/test/rotten && unzip -j test_rotten_images.zip"
      ],
      "metadata": {
        "id": "gOoxJ2B4ECtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "wseUCgG6QUXn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dq9Lp9Z9wJEd"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 256\n",
        "RANDOM_SEED = 42\n",
        "TRAIN_SIZE, VAL_SIZE = 0.8, 0.2\n",
        "IMG_SIZE = 320 # Original images are ~400*400 px so resizing them to 320 retains detail while reducing computational cost\n",
        "EPOCHS = 40\n",
        "WARMUP_DUR = 10 # num of warmup epochs\n",
        "LR_MIN = 0.0001 # minimum learning rate\n",
        "MILD_DROPOUT_RATE = 0.10"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "Ol81UPOCQc6P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MXmvl6h6stv_"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Mild preprocessing only. No MixUp, CutMix, RandomCrop, or ColorJitter because a lightweight model like this one\n",
        "    is less likely to overfit. Also, this model needs to perform fine-grained classification,\n",
        "    so aggressive augmentations could distort the small image regions (like signs of fungi or discoloration)\n",
        "    that are crucial for accurate prediction.\n",
        "\"\"\"\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation([-180, 180]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data into Dataloader"
      ],
      "metadata": {
        "id": "WFRogiAGQhks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GQ_x5765vvpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f004e0a-6173-4a04-a815-ab4e8aa3e9f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fresh': 0, 'rotten': 1} \n",
            " {'fresh': 0, 'rotten': 1}\n"
          ]
        }
      ],
      "source": [
        "path_to_train_imgs = \"/content/dataset/train\"\n",
        "path_to_test_imgs = \"/content/dataset/test\"\n",
        "\n",
        "full_train_data = ImageFolder(path_to_train_imgs, transform=train_transform)\n",
        "test_data = ImageFolder(path_to_test_imgs, transform=test_transform)\n",
        "train_data, val_data = random_split(full_train_data, [TRAIN_SIZE, VAL_SIZE], generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
        "\n",
        "print(f\"{full_train_data.class_to_idx} \\n {test_data.class_to_idx}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "x_a2E-zQxN6i"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=10)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA optimizations"
      ],
      "metadata": {
        "id": "k_hvEYYCQlGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.backends.cuda.matmul.allow_tf32 = True\n",
        "  torch.backends.cudnn.allow_tf32 = True\n",
        "  torch.backends.cuda.enable_flash_sdp(True)\n",
        "  torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "  torch.backends.cuda.enable_math_sdp(True)"
      ],
      "metadata": {
        "id": "Q74Vsb5yaKpA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Depthwise Seperable Convolution implementation"
      ],
      "metadata": {
        "id": "jmAOovOeQoIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Helper class for FreshNET. This is the implementation of the \"Depthwise seperable convolution\" as outlined in the MobileNET paper. Instances of this class are stacked\n",
        "together and each stack constitutes a hidden layer in the NN. A DepthwiseSeperableConvolution instance is structured as follows:\n",
        "\n",
        "- Pointwise convolution (applied along all channels at a single pixel) for channel expansion.\n",
        "- Depthwise convolution (applies across each channel individually) for the model to efficiently learn features in a high dimensional space.\n",
        "- Pointwise convolution to compress channels slightly.\n",
        "\n",
        "In between the convolutional layers are batch normalizations and after the final layer is an activation function.\n",
        "\"\"\"\n",
        "\n",
        "class DepthwiseSeperableConvolution(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, exp_factor, downsample_factor, kernel_size=3):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.exp_factor = exp_factor\n",
        "    self.downsample_factor = downsample_factor\n",
        "    self.kernel_size = kernel_size\n",
        "\n",
        "    out = self.in_channels * self.exp_factor\n",
        "\n",
        "    self.preserve = self.in_channels == self.out_channels\n",
        "    \"\"\"\n",
        "        batchnorm normalizes inputs which has shown to improve training. It's typically applied after the convolutional layer output and before the activation function\n",
        "        The MobileNET paper also mentions, with the exception of the final fully connected linear layer, \"all layers are followed by a batchnorm and ReLU nonlinearity\"\n",
        "    \"\"\"\n",
        "\n",
        "    self.block = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=self.in_channels, out_channels=out, kernel_size=1, stride=1, groups=1), # expansion\n",
        "        nn.BatchNorm2d(out),\n",
        "\n",
        "        # adding padding because the image downsamples, but skip connections require same size\n",
        "        nn.Conv2d(in_channels=out, out_channels=out, kernel_size=self.kernel_size, padding=self.kernel_size // 2, stride=self.downsample_factor, groups=out), # depthwise\n",
        "        nn.BatchNorm2d(out),\n",
        "        nn.Conv2d(in_channels=out, out_channels=self.out_channels, kernel_size=1, stride=1, groups=1), # pointwise\n",
        "        nn.BatchNorm2d(self.out_channels),\n",
        "\n",
        "        nn.ReLU6(), # output ∈ [0, 6] ==> more efficient than regular ReLU\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.block(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "VbsxVpDvSGbR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skip connection"
      ],
      "metadata": {
        "id": "N845-3DoQrJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a simple skip connection implementation\n",
        "\n",
        "\"\"\"\n",
        "FreshNET only applies skip connections between DepthwiseSeperableConvolution instances which preserve channel dimension in order to adhere to the laws of vector addition.\n",
        "\"\"\"\n",
        "class SkipConnection(nn.Module):\n",
        "  def __init__(self, seq): # seq is an nn.Sequential instance\n",
        "    super().__init__()\n",
        "\n",
        "    self.seq = seq\n",
        "\n",
        "    for module in seq:\n",
        "      if not hasattr(module, 'preserve'):\n",
        "        raise Exception(\"Cannot apply skip connection between layers of different channel dimensions\")\n",
        "\n",
        "    self.seq = seq\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.gradient = x\n",
        "\n",
        "    for depthwise_sep_conv in self.seq:\n",
        "      x = depthwise_sep_conv(x)\n",
        "\n",
        "    return x + self.gradient"
      ],
      "metadata": {
        "id": "z3wLK60ji3mg"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FreshNet: A MobileNET adaptation"
      ],
      "metadata": {
        "id": "5dYcj9FeQt25"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BDb7UN4x0Z0z"
      },
      "outputs": [],
      "source": [
        "# A MobileNET adaptation\n",
        "class FreshNET(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  FreshNET applies the concept of depthwise seperable convolutions as mentioned in the MobileNet paper, but the image dimensions are slightly increased\n",
        "  to better suit the dataset and leverage higher image quality. Specifically, FreshNET decouples spatial filtering and channel mixing,\n",
        "  the two operations traditionally combined in standard convolutions, and performs them as separate, more efficient operations. As the authors of MobileNet noted,\n",
        "  it \"drastically reduc[es] computation and model size\". Despite this computational efficiency, accuracy is largely preserved -- the original MobileNet paper reports\n",
        "  only about a 1% drop in accuracy compared to standard convolutions.\n",
        "\n",
        "  This separation is implemented in DepthwiseSeperableConvolution class. Each instance consists of:\n",
        "  - A pointwise 1*1 convolution for channel expansion, increasing feature dimensionality.\n",
        "  - A depthwise 3*3 convolution applied independently per channel for spatial filtering.\n",
        "  - Another pointwise convolution to project back to a lower-dimensional space.\n",
        "\n",
        "  This design significantly reduces computational cost while maintaining high representational power. There are 10 layers, of which 7 are DepthwiseSeperableConvolution layers.\n",
        "  Each DepthwiseSeperableConvolution layer consists of 1-4 DepthwiseSeperableConvolution instances. Within the DepthwiseSeperableConvolution layer, there are non-linear activation functions.\n",
        "\n",
        "  Skip connections are also applied where possible to improve gradient flow and preserve fine-grained information. For a task like fresh/rotten classification,\n",
        "  fine-grained information can be very valuable.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # SiLU is good for vanishing gradients and can be applied in the later layers where ReLU might not be as effective (dead neurons)\n",
        "    silu = nn.SiLU()\n",
        "\n",
        "    # Regularization\n",
        "    \"\"\"\n",
        "    The original paper notes \"we use less regularization and data augmentation techniques because small models have less trouble with overfitting\".\n",
        "    This is why I chose to use dropout with 15% probability since that's on the lower end of the probability spectrum.\n",
        "    \"\"\"\n",
        "    dropout_mild = nn.Dropout(MILD_DROPOUT_RATE)\n",
        "\n",
        "    # initial regular convolution\n",
        "    initial_conv = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2) # 320*320*3 --> 160*160*32\n",
        "\n",
        "    # inverted residual block 1: 160*160*32 --> 160*160*16, channel expansion = 1\n",
        "    block_1 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(32, 16, 1, 1)\n",
        "    )\n",
        "\n",
        "    # inverted residual block 2: 160*160*16 --> 80*80*24, channel expansion = 6\n",
        "    block_2 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(16, 24, 6, 2),\n",
        "        DepthwiseSeperableConvolution(24, 24, 6, 1)\n",
        "    )\n",
        "\n",
        "    # inverted residual block 3: 80*80*24 --> 40*40*32, channel expansion = 6\n",
        "    block_3 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(24, 32, 6, 2),\n",
        "        SkipConnection(nn.Sequential(\n",
        "              DepthwiseSeperableConvolution(32, 32, 6, 1),\n",
        "              DepthwiseSeperableConvolution(32, 32, 6, 1),\n",
        "          )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # inverted residual block 4: 40*40*32 --> 20*20*64, channel expansion = 6\n",
        "    block_4 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(32, 64, 6, 2),\n",
        "        SkipConnection(nn.Sequential(\n",
        "              DepthwiseSeperableConvolution(64, 64, 6, 1),\n",
        "              DepthwiseSeperableConvolution(64, 64, 6, 1),\n",
        "              DepthwiseSeperableConvolution(64, 64, 6, 1),\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # inverted residual block 5: 20*20*64 --> 20*20*96, channel expansion = 6\n",
        "    block_5 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(64, 96, 6, 1),\n",
        "        SkipConnection(nn.Sequential(\n",
        "              DepthwiseSeperableConvolution(96, 96, 6, 1),\n",
        "              DepthwiseSeperableConvolution(96, 96, 6, 1),\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # inverted residual block 6: 20*20*96 --> 10*10*160, channel expansion = 6\n",
        "    block_6 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(96, 160, 6, 2),\n",
        "        SkipConnection(nn.Sequential(\n",
        "              DepthwiseSeperableConvolution(160, 160, 6, 1),\n",
        "              DepthwiseSeperableConvolution(160, 160, 6, 1),\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # inverted residual block 7: 10*10*160 --> 10*10*320, channel expansion = 6\n",
        "    block_7 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(160, 320, 6, 1)\n",
        "    )\n",
        "\n",
        "    # final regular convolution\n",
        "    final_conv = nn.Conv2d(in_channels=320, out_channels=1280, kernel_size=1, stride=1) # 10*10*320 --> 10*10*1280\n",
        "\n",
        "    \"\"\"\n",
        "      MaxPooling is often preferred for highlighting dominant features, but it can be too aggressive in lightweight models\n",
        "      like MobileNet or EfficientNet, which already have low spatial resolution. It can discard valuable spatial\n",
        "      information. Average Pooling computes the average of nearby pixels, preserving more context. This makes it more\n",
        "      suitable for compact architectures like this one.\n",
        "    \"\"\"\n",
        "\n",
        "    avg_pool = nn.AvgPool2d(10) # 10*10*1280 --> 1*1*1280\n",
        "    flatten = nn.Flatten() # fc layers expect a single tensor not a feature map\n",
        "\n",
        "    # fully connected layers\n",
        "    \"\"\"\n",
        "    The MobileNET paper mapped from 1280 directly to 1000 since it was being trained on ImageNET. My dataset, however, only has 2\n",
        "    output classes: fresh (0) or rotten (1). Mapping directly from 1280 to 2 is an abrupt jump which could limit the model's ability to\n",
        "    distinguish between classes, so I introduced an additional layer to map from 1280 to 500 then applied nonlinearity and\n",
        "    mild dropout before going from 500 to 2.\n",
        "    \"\"\"\n",
        "    fc_1 = nn.Linear(in_features=1280, out_features=500)\n",
        "    fc_2 = nn.Linear(in_features=500, out_features=2)\n",
        "\n",
        "    \"\"\"\n",
        "    The sequence of layers is as follows:\n",
        "\n",
        "      - intial regular convolution\n",
        "      - 4 stacks of DepthwiseSeperableConvolution\n",
        "      - mild dropout for regularization\n",
        "      - 3 more stacks of DepthwiseSeperableConvolution\n",
        "      - 1 final regular convolution\n",
        "      - average pooling\n",
        "      - activation function\n",
        "      - first fully connected linear layer\n",
        "      - activation function\n",
        "      - mild dropout for regularization\n",
        "      - final fully connected linear layer\n",
        "\n",
        "    There are skip connections in blocks 3-6.\n",
        "    \"\"\"\n",
        "\n",
        "    self.layers = nn.Sequential(\n",
        "        initial_conv,\n",
        "\n",
        "        block_1,\n",
        "        block_2,\n",
        "        block_3,\n",
        "        block_4,\n",
        "        dropout_mild,\n",
        "\n",
        "        block_5,\n",
        "        block_6,\n",
        "        block_7,\n",
        "\n",
        "        final_conv,\n",
        "        avg_pool,\n",
        "        flatten,\n",
        "        silu,\n",
        "        dropout_mild,\n",
        "\n",
        "        fc_1,\n",
        "        silu,\n",
        "\n",
        "        fc_2\n",
        "    )\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "GIaS73iFUUBE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ZCIQf4NUxhgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0ad5c3e-a657-4b62-c193-c095cc97812f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-19-96917922.py:15: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=WARMUP_DUR, eta_min=LR_MIN, max_epochs=EPOCHS)\n"
          ]
        }
      ],
      "source": [
        "cnn = FreshNET()\n",
        "cnn.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss() # std loss function for classification\n",
        "\n",
        "optimizer = optim.Adam(cnn.parameters()) # RMSProp is pretty sensitive to changes in LR so i wanted to try Adam\n",
        "\n",
        "scaler = torch.amp.GradScaler()\n",
        "\n",
        "\"\"\"\n",
        "Not mentioned in the original paper, but this scheduler was used because linear warmup reduces volatility in the earlier epochs.\n",
        "Cosine annealing can improve training stability and convergence and declining LR steadily means the model will rely more on the features\n",
        "it learns early on -- the key differentiators -- rather than picking up potential noise and overfitting.\n",
        "\"\"\"\n",
        "scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=WARMUP_DUR, eta_min=LR_MIN, max_epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and validate"
      ],
      "metadata": {
        "id": "afXvV6nhQynV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(cnn):\n",
        "  for i in range(EPOCHS):\n",
        "    torch.cuda.empty_cache()\n",
        "    correct, total = 0, 0\n",
        "    running_loss = 0\n",
        "\n",
        "    cnn.train(True)\n",
        "\n",
        "    loop = tqdm(train_dataloader, desc=f\"Epoch {i+1}/{EPOCHS}\", leave=True, disable=False)\n",
        "\n",
        "    for j, (input, labels) in enumerate(loop, 1):\n",
        "        input, labels = input.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "          output = cnn(input)\n",
        "          loss = loss_fn(output, labels)\n",
        "          running_loss += loss.item()\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        avg_loss = running_loss / j\n",
        "        loop.set_postfix(loss=avg_loss)\n",
        "\n",
        "        pred = torch.argmax(output, dim=1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (pred == labels).sum().item()\n",
        "\n",
        "\n",
        "    cnn.eval()\n",
        "    val_total, val_correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for image, label in val_dataloader:\n",
        "            image, label = image.to(device), label.to(device)\n",
        "\n",
        "            with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
        "              output = cnn(image)\n",
        "\n",
        "            pred = torch.argmax(output, dim=1)\n",
        "            val_total += label.size(0)\n",
        "            val_correct += (pred == label).sum().item()\n",
        "\n",
        "        val_accuracy = val_correct / val_total\n",
        "        accuracy = correct / total\n",
        "\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Epoch {i + 1}: LR={current_lr:.6f} \\t Train Acc: {accuracy:.4f} \\t Val Acc: {val_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "pMzwv9TEVZCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(cnn)"
      ],
      "metadata": {
        "id": "yFiUCT5I6g64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "de4e2604-0ff5-4045-fb63-346585736100",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/40: 100%|██████████| 74/74 [03:26<00:00,  2.80s/it, loss=0.694]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: LR=0.000111 \t Train Acc: 0.4751 \t Val Acc: 0.4711\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/40: 100%|██████████| 74/74 [00:26<00:00,  2.74it/s, loss=0.419]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: LR=0.000222 \t Train Acc: 0.7976 \t Val Acc: 0.8721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/40: 100%|██████████| 74/74 [00:27<00:00,  2.71it/s, loss=0.279]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: LR=0.000333 \t Train Acc: 0.8914 \t Val Acc: 0.8747\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/40: 100%|██████████| 74/74 [00:27<00:00,  2.71it/s, loss=0.254]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: LR=0.000444 \t Train Acc: 0.9003 \t Val Acc: 0.8503\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/40: 100%|██████████| 74/74 [00:27<00:00,  2.66it/s, loss=0.218]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: LR=0.000556 \t Train Acc: 0.9153 \t Val Acc: 0.8973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/40: 100%|██████████| 74/74 [00:27<00:00,  2.73it/s, loss=0.216]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: LR=0.000667 \t Train Acc: 0.9143 \t Val Acc: 0.9176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/40: 100%|██████████| 74/74 [00:27<00:00,  2.72it/s, loss=0.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: LR=0.000778 \t Train Acc: 0.9170 \t Val Acc: 0.8694\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/40: 100%|██████████| 74/74 [00:27<00:00,  2.70it/s, loss=0.199]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: LR=0.000889 \t Train Acc: 0.9178 \t Val Acc: 0.8935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/40: 100%|██████████| 74/74 [00:27<00:00,  2.69it/s, loss=0.183]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: LR=0.001000 \t Train Acc: 0.9246 \t Val Acc: 0.9263\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/40: 100%|██████████| 74/74 [00:27<00:00,  2.71it/s, loss=0.169]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: LR=0.001000 \t Train Acc: 0.9310 \t Val Acc: 0.9049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/40: 100%|██████████| 74/74 [00:27<00:00,  2.73it/s, loss=0.148]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: LR=0.000998 \t Train Acc: 0.9403 \t Val Acc: 0.9339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/40: 100%|██████████| 74/74 [00:27<00:00,  2.71it/s, loss=0.136]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: LR=0.000990 \t Train Acc: 0.9492 \t Val Acc: 0.9291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/40: 100%|██████████| 74/74 [00:27<00:00,  2.67it/s, loss=0.121]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: LR=0.000978 \t Train Acc: 0.9541 \t Val Acc: 0.9208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/40: 100%|██████████| 74/74 [00:27<00:00,  2.70it/s, loss=0.123]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: LR=0.000961 \t Train Acc: 0.9527 \t Val Acc: 0.9553\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/40: 100%|██████████| 74/74 [00:26<00:00,  2.77it/s, loss=0.113]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: LR=0.000940 \t Train Acc: 0.9559 \t Val Acc: 0.9358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/40: 100%|██████████| 74/74 [00:27<00:00,  2.67it/s, loss=0.109]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: LR=0.000914 \t Train Acc: 0.9594 \t Val Acc: 0.9388\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/40: 100%|██████████| 74/74 [00:27<00:00,  2.69it/s, loss=0.105]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17: LR=0.000884 \t Train Acc: 0.9600 \t Val Acc: 0.9627\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/40:  93%|█████████▎| 69/74 [00:25<00:01,  3.35it/s, loss=0.0858]Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7abc1874c360>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1582, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 948, in wait\n",
            "    ready = selector.select(timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/selectors.py\", line 415, in select\n",
            "    fd_event_list = self._selector.poll(timeout)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt: \n",
            "Epoch 18/40:  93%|█████████▎| 69/74 [00:26<00:01,  2.65it/s, loss=0.0858]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-20-1329408664.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m           \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantizing model\n",
        "\n",
        "Quantization Aware Training (QAT) involves training the model with full floating point precision, float32, while simulating the effects of int8 quantization. This 'fake' quantization makes the model well-equipped to perform under quantization, maintaining accuracy better than if it used post-training  quantization instead.\n",
        "\n",
        "Ideally, I would choose FX Graph mode because PyTorch handles most of the quantization internally, but as of writing this, PyTorch does not support Conv2d instances with groups != 1 on FX Graph mode. Also, PyTorch currently does not support EagerMode QAT on CUDA, only on CPU which is highly inefficient for a deep learning model like this. This leaves only Export quantization mode which seems to sit somewhere in between FX graph mode and EagerMode in terms of how much quantization is internally managed by PyTorch."
      ],
      "metadata": {
        "id": "8oDdlacxT0dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "copied_cnn = deepcopy(cnn) # deepcopying cnn so we retain the original just in case\n",
        "example_inputs, _ = next(iter(train_dataloader)) # getting an example input for the frozen model\n",
        "copied_cnn = torch.export.export(copied_cnn, example_inputs).module() # exporting model\n",
        "quantizer = XNNPACKQuantizer().set_global(get_symmetric_quantization_config()) # creating a quanitzer object\n",
        "prepared_cnn = prepare_qat_pt2e(copied_cnn, quantizer) # quantizing"
      ],
      "metadata": {
        "id": "bV3ULaKrT2u2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "1e4f5439-5614-4baa-e9bd-5be3d78ad2a0"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UserError",
          "evalue": "Expecting `args` to be a tuple of example positional inputs, got <class 'torch.Tensor'>",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUserError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-22-3049196608.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcopied_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# deepcopying cnn so we retain the original just in case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# getting an example input for the frozen model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcopied_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopied_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# exporting model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mquantizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXNNPACKQuantizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_global\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_symmetric_quantization_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# creating a quanitzer object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprepared_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_qat_pt2e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopied_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# quantizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/export/__init__.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0;34m\"using `TS2EPConverter(mod, args, kwargs).convert()` instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         )\n\u001b[0;32m--> 368\u001b[0;31m     return _export(\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/export/_trace.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m                     \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_EXPORT_FLAGS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 )\n\u001b[0;32m-> 1035\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0m_EXPORT_FLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/export/_trace.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m             \u001b[0mep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m             log_export_usage(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/export/exported_program.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0munset_fake_temporarily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/export/_trace.py\u001b[0m in \u001b[0;36m_export\u001b[0;34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature, pre_dispatch, allow_complex_guards_as_runtime_asserts, _is_torch_jit_trace)\u001b[0m\n\u001b[1;32m   1968\u001b[0m     \u001b[0;31m# while internally it returns False UNLESS otherwise specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1969\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mexport_training_ir_rollout_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1970\u001b[0;31m         return _export_for_training(\n\u001b[0m\u001b[1;32m   1971\u001b[0m             \u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1972\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/export/_trace.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1033\u001b[0m                     \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_EXPORT_FLAGS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 )\n\u001b[0;32m-> 1035\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0m_EXPORT_FLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/export/_trace.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m             \u001b[0mep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m             log_export_usage(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/export/exported_program.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0munset_fake_temporarily\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/export/_trace.py\u001b[0m in \u001b[0;36m_export_for_training\u001b[0;34m(mod, args, kwargs, dynamic_shapes, strict, preserve_module_call_signature)\u001b[0m\n\u001b[1;32m   1819\u001b[0m         \u001b[0moriginal_state_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m         \u001b[0mdynamic_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1821\u001b[0;31m     ) = _process_export_inputs(mod, args, kwargs, dynamic_shapes)\n\u001b[0m\u001b[1;32m   1822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m     export_func = (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/export/_trace.py\u001b[0m in \u001b[0;36m_process_export_inputs\u001b[0;34m(mod, args, kwargs, dynamic_shapes)\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1075\u001b[0;31m         raise UserError(\n\u001b[0m\u001b[1;32m   1076\u001b[0m             \u001b[0mUserErrorType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINVALID_INPUT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m             \u001b[0;34mf\"Expecting `args` to be a tuple of example positional inputs, got {type(args)}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUserError\u001b[0m: Expecting `args` to be a tuple of example positional inputs, got <class 'torch.Tensor'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_cnn = convert_pt2e(prepared_cnn)"
      ],
      "metadata": {
        "id": "yGKWeiWzVut6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "b8daa8bc-126b-44e4-9da6-0215bfff2bff"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/utils.py:408: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Quantized cudnn conv2d is currently limited to groups = 1; received groups =32",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-21-3267036264.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquantized_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_fx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared_cnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/quantize_fx.py\u001b[0m in \u001b[0;36mconvert_fx\u001b[0;34m(graph_module, convert_custom_config, _remove_qconfig, qconfig_mapping, backend_config)\u001b[0m\n\u001b[1;32m    610\u001b[0m     \"\"\"\n\u001b[1;32m    611\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quantization_api.quantize_fx.convert_fx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m     return _convert_fx(\n\u001b[0m\u001b[1;32m    613\u001b[0m         \u001b[0mgraph_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0mis_reference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/quantize_fx.py\u001b[0m in \u001b[0;36m_convert_fx\u001b[0;34m(graph_module, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig, qconfig_mapping, backend_config, is_decomposed)\u001b[0m\n\u001b[1;32m    538\u001b[0m     }\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m     quantized = convert(\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0mgraph_module\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mis_reference\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/fx/convert.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(model, is_reference, convert_custom_config, is_standalone_module, _remove_qconfig_flag, qconfig_mapping, backend_config, is_decomposed)\u001b[0m\n\u001b[1;32m   1252\u001b[0m     \u001b[0;31m# TODO: maybe move this to quantize_fx.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_reference\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlower_to_fbgemm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_name_to_qconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_name_to_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m     \u001b[0;31m# TODO: this looks hacky, we want to check why we need this and see if we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/fx/lower_to_fbgemm.py\u001b[0m in \u001b[0;36mlower_to_fbgemm\u001b[0;34m(model, qconfig_map, node_name_to_scope)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mto\u001b[0m \u001b[0mfbgemm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \"\"\"\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_lower_to_native_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqconfig_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_name_to_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/fx/_lower_to_native_backend.py\u001b[0m in \u001b[0;36m_lower_to_native_backend\u001b[0;34m(model, qconfig_map, node_name_to_scope)\u001b[0m\n\u001b[1;32m   1306\u001b[0m     \u001b[0moperator\u001b[0m \u001b[0msignature\u001b[0m \u001b[0mso\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mlowered\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m     \"\"\"\n\u001b[0;32m-> 1308\u001b[0;31m     \u001b[0m_lower_static_weighted_ref_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqconfig_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1309\u001b[0m     \u001b[0m_lower_static_weighted_ref_module_with_two_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqconfig_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1310\u001b[0m     \u001b[0m_lower_dynamic_weighted_ref_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/quantization/fx/_lower_to_native_backend.py\u001b[0m in \u001b[0;36m_lower_static_weighted_ref_module\u001b[0;34m(model, qconfig_map)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0moutput_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0moutput_zero_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_point_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mq_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_zero_point\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;31m# replace reference module with quantized module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0mparent_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parent_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_node\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/nn/quantized/modules/conv.py\u001b[0m in \u001b[0;36mfrom_reference\u001b[0;34m(cls, ref_qconv, output_scale, output_zero_point)\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0moutput_zero_point\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mzero\u001b[0m \u001b[0mpoint\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \"\"\"\n\u001b[0;32m--> 333\u001b[0;31m         qconv = cls(\n\u001b[0m\u001b[1;32m    334\u001b[0m             \u001b[0mref_qconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0mref_qconv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/nn/quantized/modules/conv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;31m# Subclasses of _ConvNd need to call _init rather than __init__. See\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;31m# discussion on PR #49702\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         super()._init(\n\u001b[0m\u001b[1;32m    548\u001b[0m             \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/nn/quantized/modules/conv.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, device, dtype)\u001b[0m\n\u001b[1;32m    114\u001b[0m         )\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weight_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/ao/nn/quantized/modules/conv.py\u001b[0m in \u001b[0;36mset_weight_bias\u001b[0;34m(self, w, b)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_weight_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"zeros\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             self._packed_params = torch.ops.quantized.conv2d_prepack(\n\u001b[0m\u001b[1;32m    568\u001b[0m                 \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_torchbind_op_overload\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_must_dispatch_in_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_call_overload_packet_from_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;31m# TODO: use this to make a __dir__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Quantized cudnn conv2d is currently limited to groups = 1; received groups =32"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model size (no quantization)"
      ],
      "metadata": {
        "id": "Gc5_kJ5ebGJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_size = 0\n",
        "for param in cnn.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "for buffer in cnn.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('model size: {:.3f}MB'.format(size_all_mb))"
      ],
      "metadata": {
        "id": "GJVsl5KsZ7Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model size (with quantization)\n"
      ],
      "metadata": {
        "id": "1A0yeJyQbL6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_size = 0\n",
        "for param in quantized_cnn.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "buffer_size = 0\n",
        "for buffer in quantized_cnn.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('model size: {:.3f}MB'.format(size_all_mb))"
      ],
      "metadata": {
        "id": "VpEuRfIQKtiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "WZUAMYaCQ7rT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(test_dataloader):\n",
        "  accuracy = 0\n",
        "  total = 0\n",
        "\n",
        "  for batch in test_dataloader:\n",
        "    images, labels = batch\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    predicted = torch.argmax(cnn(images), dim=1)\n",
        "    accuracy += (predicted == labels).sum().item()\n",
        "    total += labels.size(0)\n",
        "\n",
        "  avg_acc = accuracy / total\n",
        "\n",
        "  return avg_acc"
      ],
      "metadata": {
        "id": "Tk4rp3ox4I7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "    test_acc = test(test_dataloader)\n",
        "    print(f\"Final test accuracy: {100 * test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "KY-5zlw17L-I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMut5y/QMrcW+4omtpadcfZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}