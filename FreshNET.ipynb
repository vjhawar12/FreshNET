{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vjhawar12/FreshNET-A-mobileNET-adaptation/blob/main/FreshNET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "VD5TbE7WQKdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install lightning-bolts"
      ],
      "metadata": {
        "id": "9DCFEjr1bUQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2dEyze5_tgqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d38cd01a-c4b0-41a9-ee6b-c4e467866413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pl_bolts/__init__.py:11: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(numpy, tp_name):\n",
            "/usr/local/lib/python3.11/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:34: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
            "/usr/local/lib/python3.11/dist-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:92: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
            "/usr/local/lib/python3.11/dist-packages/pl_bolts/losses/self_supervised_learning.py:228: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  self.nce_loss = AmdimNCELoss(tclip)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "from tqdm import tqdm\n",
        "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
        "from copy import deepcopy\n",
        "from torch.ao.quantization import QuantStub, DeQuantStub, prepare_qat, get_default_qat_qconfig, convert, fuse_modules"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading dataset from Google Cloud Storage"
      ],
      "metadata": {
        "id": "PLDhrdLkQOgW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth login"
      ],
      "metadata": {
        "id": "Z6kjxCQBG33V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e011c6ac-a872-415a-edcc-1d76f9cdbd17"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=GSR4ESjldk7xd8bEnHANy3zILxt7qY&prompt=consent&token_usage=remote&access_type=offline&code_challenge=P5btcZDVKD1-I5Silp08YC1TJVSpZ4fUOzqBzJz_pEE&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0AVMBsJhFRpjFHfD0TPGKTTRwyhfPFAXcB7TEtPdJKaqp46-kQFkfALnCFhecSEP1foKJOg\n",
            "\n",
            "You are now logged in as [vedantj1203@gmail.com].\n",
            "Your current project is [freshnet-466505].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "id": "xpxNoZaCCgZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfd86e17-8312-4533-8856-e6d7f67252f8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=h9JRgbYNEBmExPwN9GYLxZXdKLeByu&prompt=consent&token_usage=remote&access_type=offline&code_challenge=6FNGRmV674yAwDEnyg2mo2Caehu2TEQfrlxn5Zm9bSA&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0AVMBsJhoSXjxZrpDpiCmkoIiICg_GNIeGf-WgpEEOe6ACcXKCQ627iPLZUFPi_Wq50o2OQ\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\n",
            "Quota project \"freshnet-466505\" was added to ADC which can be used by Google client libraries for billing and quota. Note that some services may still bill the project owning the resource.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6I26bJ0bd0ck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb827575-0536-4584-b702-351c3c077742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ],
      "source": [
        "!gcloud config set project freshnet-466505"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_bucket_with_transfer_manager(bucket_name, destination_directory=\"\", workers=8, max_results=1000):\n",
        "\n",
        "    from google.cloud.storage import Client, transfer_manager\n",
        "\n",
        "    client = Client()\n",
        "    bucket = client.bucket(bucket_name)\n",
        "\n",
        "    blob_names = [blob.name for blob in bucket.list_blobs(max_results=max_results)]\n",
        "\n",
        "    results = transfer_manager.download_many_to_path(\n",
        "        bucket, blob_names, destination_directory=destination_directory, max_workers=workers\n",
        "    )\n",
        "\n",
        "    for name, result in zip(blob_names, results):\n",
        "        if isinstance(result, Exception):\n",
        "            print(\"Failed to download {} due to exception: {}\".format(name, result))\n",
        "        else:\n",
        "            print(\"Downloaded {} to {}.\".format(name, destination_directory + name))\n",
        "\n",
        "download_bucket_with_transfer_manager(\"freshnet-images\", destination_directory=\"/content/dataset/\", workers=8, max_results=None)"
      ],
      "metadata": {
        "id": "gVKqs6cjAvE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "608b31b6-87ca-4a17-c5d1-a00fcf22ef8e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download test/ due to exception: [Errno 21] Is a directory: '/content/dataset/test/'\n",
            "Failed to download test/fresh/ due to exception: [Errno 21] Is a directory: '/content/dataset/test/fresh/'\n",
            "Downloaded test/fresh/test_fresh_images.zip to /content/dataset/test/fresh/test_fresh_images.zip.\n",
            "Failed to download test/rotten/ due to exception: [Errno 21] Is a directory: '/content/dataset/test/rotten/'\n",
            "Downloaded test/rotten/test_rotten_images.zip to /content/dataset/test/rotten/test_rotten_images.zip.\n",
            "Failed to download train/ due to exception: [Errno 21] Is a directory: '/content/dataset/train/'\n",
            "Failed to download train/fresh/ due to exception: [Errno 21] Is a directory: '/content/dataset/train/fresh/'\n",
            "Downloaded train/fresh/train_fresh_images.zip to /content/dataset/train/fresh/train_fresh_images.zip.\n",
            "Failed to download train/rotten/ due to exception: [Errno 21] Is a directory: '/content/dataset/train/rotten/'\n",
            "Downloaded train/rotten/train_rotten_images.zip to /content/dataset/train/rotten/train_rotten_images.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/dataset/train/rotten && unzip -j train_rotten_images.zip"
      ],
      "metadata": {
        "id": "1ePoZVvuDYGf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "045143bd-f885-4142-e041-2f2eb45594cd"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  train_rotten_images.zip\n",
            "replace p_r367.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/dataset/train/fresh && unzip -j train_fresh_images.zip"
      ],
      "metadata": {
        "id": "BNDkmH8NDxwt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e8ea7e4-bce2-4a9e-f452-557b3970b541"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  train_fresh_images.zip\n",
            "replace b_f882.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/dataset/test/fresh && unzip -j test_fresh_images.zip"
      ],
      "metadata": {
        "id": "AkCehLpLD_Z5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea82f207-f4c8-45b8-9df3-b4742cc21022"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  test_fresh_images.zip\n",
            "replace b_f304.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/dataset/test/rotten && unzip -j test_rotten_images.zip"
      ],
      "metadata": {
        "id": "gOoxJ2B4ECtG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ed46068-7155-41cb-e3de-e63daa8b1849"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  test_rotten_images.zip\n",
            "replace p_r367.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "wseUCgG6QUXn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "dq9Lp9Z9wJEd"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 256\n",
        "RANDOM_SEED = 42\n",
        "TRAIN_SIZE, VAL_SIZE = 0.8, 0.2\n",
        "IMG_SIZE = 320 # Original images are ~400*400 px so resizing them to 320 retains detail while reducing computational cost\n",
        "EPOCHS = 30\n",
        "WARMUP_DUR = 10 # num of warmup epochs\n",
        "LR_MIN = 0.0001 # minimum learning rate\n",
        "MILD_DROPOUT_RATE = 0.10 # mild enough to avoid overfitting on a small dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "Ol81UPOCQc6P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "MXmvl6h6stv_"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Mild preprocessing only. No MixUp, CutMix, RandomCrop, or ColorJitter because a lightweight model like this one\n",
        "    is less likely to overfit. Also, this model needs to perform fine-grained classification,\n",
        "    so aggressive augmentations could distort the small image regions (like signs of fungi or discoloration)\n",
        "    that are crucial for accurate prediction.\n",
        "\"\"\"\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation([-180, 180]),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data into Dataloader"
      ],
      "metadata": {
        "id": "WFRogiAGQhks"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GQ_x5765vvpn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51cd801f-1f61-46be-d994-143523325238"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'fresh': 0, 'rotten': 1} \n",
            " {'fresh': 0, 'rotten': 1}\n"
          ]
        }
      ],
      "source": [
        "path_to_train_imgs = \"/content/dataset/train\"\n",
        "path_to_test_imgs = \"/content/dataset/test\"\n",
        "\n",
        "full_train_data = ImageFolder(path_to_train_imgs, transform=train_transform)\n",
        "test_data = ImageFolder(path_to_test_imgs, transform=test_transform)\n",
        "train_data, val_data = random_split(full_train_data, [TRAIN_SIZE, VAL_SIZE], generator=torch.Generator().manual_seed(RANDOM_SEED))\n",
        "\n",
        "print(f\"{full_train_data.class_to_idx} \\n {test_data.class_to_idx}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "x_a2E-zQxN6i"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=10)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "indices_to_keep = [i for i, (image, label) in enumerate(test_data) if i < 20]\n",
        "mini = Subset(test_data, indices_to_keep)\n",
        "mini_test_dataloader = DataLoader(mini, batch_size=5, shuffle=False, num_workers=4)\n",
        "\n",
        "indices_to_keep = [i for i, (image, label) in enumerate(test_data) if i > 20 and i < 40]\n",
        "calibrate = Subset(test_data, indices_to_keep)\n",
        "calibrate_dataloader = DataLoader(calibrate, shuffle=False, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CUDA optimizations"
      ],
      "metadata": {
        "id": "k_hvEYYCQlGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  torch.backends.cuda.matmul.allow_tf32 = True\n",
        "  torch.backends.cudnn.allow_tf32 = True\n",
        "  torch.backends.cuda.enable_flash_sdp(True)\n",
        "  torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
        "  torch.backends.cuda.enable_math_sdp(True)"
      ],
      "metadata": {
        "id": "Q74Vsb5yaKpA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization helper class: Float Wrapper\n",
        "\n",
        "This class wraps operations that Pytorch cannot quantize, such as deptwise convolutions with groups != 1, with a DeQuantStub and a QuantStub. This ensures during quantization the particular operations wrapped by a FloatWrapper instance receive float32 objects and not INT8."
      ],
      "metadata": {
        "id": "GPSUbTvGRN3A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FloatWrapper(nn.Module):\n",
        "  def __init__(self, layer):\n",
        "    super().__init__()\n",
        "    self.layer = layer\n",
        "    self.quant = QuantStub()\n",
        "    self.dequant = DeQuantStub()\n",
        "    self.qconfig = None\n",
        "    self.layer.qconfig = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.dequant(x)\n",
        "    x = self.layer(x)\n",
        "    x = self.quant(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "R--PMV9TRsgw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Depthwise Seperable Convolution implementation"
      ],
      "metadata": {
        "id": "jmAOovOeQoIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Sequential):\n",
        "  def __init__(self, in_channels, out, out_channels, kernel_size, downsample_factor):\n",
        "    super().__init__(\n",
        "      # Fusing these 3\n",
        "      nn.Conv2d(in_channels=in_channels, out_channels=out, kernel_size=1, stride=1, groups=1), # expansion\n",
        "      nn.BatchNorm2d(out),\n",
        "      nn.ReLU(),\n",
        "\n",
        "      FloatWrapper(\n",
        "          # adding padding because the image downsamples, but skip connections require same size\n",
        "          nn.Conv2d(in_channels=out, out_channels=out, kernel_size=kernel_size, padding=kernel_size // 2, stride=downsample_factor, groups=out)\n",
        "          # depthwise\n",
        "      ),\n",
        "\n",
        "      # Fusing these 3\n",
        "      nn.Conv2d(in_channels=out, out_channels=out_channels, kernel_size=1, stride=1, groups=1), # pointwise\n",
        "      nn.BatchNorm2d(out_channels),\n",
        "      nn.ReLU(),\n",
        "    )\n",
        ""
      ],
      "metadata": {
        "id": "WnPhaqGgv9yU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DepthwiseSeperableConvolution(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, exp_factor, downsample_factor, kernel_size=3):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "    self.exp_factor = exp_factor\n",
        "    self.downsample_factor = downsample_factor\n",
        "    self.kernel_size = kernel_size\n",
        "\n",
        "    out = self.in_channels * self.exp_factor\n",
        "\n",
        "    self.preserve = self.in_channels == self.out_channels\n",
        "    \"\"\"\n",
        "        batchnorm normalizes inputs which has shown to improve training. It's typically applied after the convolutional layer output and before the activation function\n",
        "        The MobileNET paper also mentions, with the exception of the final fully connected linear layer, \"all layers are followed by a batchnorm and ReLU nonlinearity\"\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    self.block = Block(self.in_channels, out, self.out_channels, self.kernel_size, self.downsample_factor)\n",
        "\n",
        "    \"\"\"\n",
        "    nn.Sequential(\n",
        "        nn.Conv2d(in_channels=self.in_channels, out_channels=out, kernel_size=1, stride=1, groups=1), # expansion\n",
        "        nn.BatchNorm2d(out),\n",
        "\n",
        "        FloatWrapper(\n",
        "            # adding padding because the image downsamples, but skip connections require same size\n",
        "            nn.Conv2d(in_channels=out, out_channels=out, kernel_size=self.kernel_size, padding=self.kernel_size // 2, stride=self.downsample_factor, groups=out)\n",
        "            # depthwise\n",
        "        ),\n",
        "\n",
        "        nn.BatchNorm2d(out),\n",
        "        nn.Conv2d(in_channels=out, out_channels=self.out_channels, kernel_size=1, stride=1, groups=1), # pointwise\n",
        "        nn.BatchNorm2d(self.out_channels),\n",
        "\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "    \"\"\"\n",
        "\n",
        "  def forward(self, x):\n",
        "    for layer in self.block:\n",
        "      x = layer(x)\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "VbsxVpDvSGbR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skip connection"
      ],
      "metadata": {
        "id": "N845-3DoQrJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "FreshNET only applies skip connections between DepthwiseSeperableConvolution instances which preserve channel dimension in order to adhere to the laws of vector addition.\n",
        "\"\"\"\n",
        "class SkipConnection(nn.Module):\n",
        "  def __init__(self, seq): # seq is an nn.Sequential instance\n",
        "    super().__init__()\n",
        "    self.quant = QuantStub()\n",
        "    self.dequant = DeQuantStub()\n",
        "    self.seq = seq\n",
        "\n",
        "    for module in seq:\n",
        "      if not hasattr(module, 'preserve'):\n",
        "        raise Exception(\"Cannot apply skip connection between layers of different channel dimensions\")\n",
        "\n",
        "    self.seq = seq\n",
        "\n",
        "  def forward(self, x):\n",
        "    # addition is not supported with quantized tensors so we dequant, perform addition, then quant again\n",
        "    x = self.dequant(x)\n",
        "    self.gradient = x\n",
        "\n",
        "    for depthwise_sep_conv in self.seq:\n",
        "      x = depthwise_sep_conv(x)\n",
        "\n",
        "    x = self.quant(x + self.gradient)\n",
        "    return x"
      ],
      "metadata": {
        "id": "z3wLK60ji3mg"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FreshNet: A MobileNET adaptation"
      ],
      "metadata": {
        "id": "5dYcj9FeQt25"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "BDb7UN4x0Z0z"
      },
      "outputs": [],
      "source": [
        "# A MobileNET adaptation\n",
        "class FreshNET(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  FreshNET applies the concept of depthwise seperable convolutions as mentioned in the MobileNet paper, but the image dimensions are slightly increased\n",
        "  to better suit the dataset and leverage higher image quality. Specifically, FreshNET decouples spatial filtering and channel mixing,\n",
        "  the two operations traditionally combined in standard convolutions, and performs them as separate, more efficient operations. As the authors of MobileNet noted,\n",
        "  it \"drastically reduc[es] computation and model size\". Despite this computational efficiency, accuracy is largely preserved -- the original MobileNet paper reports\n",
        "  only about a 1% drop in accuracy compared to standard convolutions.\n",
        "\n",
        "  This separation is implemented in DepthwiseSeperableConvolution class. Each instance consists of:\n",
        "  - A pointwise 1*1 convolution for channel expansion, increasing feature dimensionality.\n",
        "  - A depthwise 3*3 convolution applied independently per channel for spatial filtering.\n",
        "  - Another pointwise convolution to project back to a lower-dimensional space.\n",
        "\n",
        "  This design significantly reduces computational cost while maintaining high representational power. There are 10 layers, of which 7 are DepthwiseSeperableConvolution layers.\n",
        "  Each DepthwiseSeperableConvolution layer consists of 1-4 DepthwiseSeperableConvolution instances. Within the DepthwiseSeperableConvolution layer, there are non-linear activation functions.\n",
        "\n",
        "  Skip connections are also applied where possible to improve gradient flow and preserve fine-grained information. For a task like fresh/rotten classification,\n",
        "  fine-grained information can be very valuable.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # QuantStub and DeQuantStub are used to go from FL32 <===> quantized\n",
        "    self.quant = QuantStub()\n",
        "    self.dequant = DeQuantStub()\n",
        "\n",
        "    # SiLU is good for vanishing gradients and can be applied in the later layers where ReLU might not be as effective (dead neurons)\n",
        "    silu = nn.SiLU()\n",
        "\n",
        "    # Regularization\n",
        "    \"\"\"\n",
        "    The original paper notes \"we use less regularization and data augmentation techniques because small models have less trouble with overfitting\".\n",
        "    This is why I chose to use dropout with 15% probability since that's on the lower end of the probability spectrum.\n",
        "    \"\"\"\n",
        "    dropout_mild = nn.Dropout(MILD_DROPOUT_RATE)\n",
        "\n",
        "    # initial regular convolution\n",
        "    block_0 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2),\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "\n",
        "    ) # 320*320*3 --> 160*160*32\n",
        "\n",
        "    # inverted residual block 1: 160*160*32 --> 160*160*16, channel expansion = 1\n",
        "    block_1 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(32, 16, 1, 1)\n",
        "    )\n",
        "\n",
        "    # inverted residual block 2: 160*160*16 --> 80*80*24, channel expansion = 6\n",
        "    block_2 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(16, 24, 6, 2),\n",
        "        DepthwiseSeperableConvolution(24, 24, 6, 1)\n",
        "    )\n",
        "\n",
        "    # inverted residual block 3: 80*80*24 --> 40*40*32, channel expansion = 6\n",
        "    block_3 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(24, 32, 6, 2),\n",
        "        SkipConnection(nn.Sequential(\n",
        "              DepthwiseSeperableConvolution(32, 32, 6, 1),\n",
        "              DepthwiseSeperableConvolution(32, 32, 6, 1),\n",
        "          )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # inverted residual block 4: 40*40*32 --> 20*20*64, channel expansion = 6\n",
        "    block_4 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(32, 64, 6, 2),\n",
        "        SkipConnection(nn.Sequential(\n",
        "              DepthwiseSeperableConvolution(64, 64, 6, 1),\n",
        "              DepthwiseSeperableConvolution(64, 64, 6, 1),\n",
        "              DepthwiseSeperableConvolution(64, 64, 6, 1),\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # inverted residual block 5: 20*20*64 --> 20*20*96, channel expansion = 6\n",
        "    block_5 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(64, 96, 6, 1),\n",
        "        SkipConnection(nn.Sequential(\n",
        "              DepthwiseSeperableConvolution(96, 96, 6, 1),\n",
        "              DepthwiseSeperableConvolution(96, 96, 6, 1),\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # inverted residual block 6: 20*20*96 --> 10*10*160, channel expansion = 6\n",
        "    block_6 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(96, 160, 6, 2),\n",
        "        SkipConnection(nn.Sequential(\n",
        "              DepthwiseSeperableConvolution(160, 160, 6, 1),\n",
        "              DepthwiseSeperableConvolution(160, 160, 6, 1),\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # inverted residual block 7: 10*10*160 --> 10*10*320, channel expansion = 6\n",
        "    block_7 = nn.Sequential(\n",
        "        DepthwiseSeperableConvolution(160, 320, 6, 1)\n",
        "    )\n",
        "\n",
        "    # final regular convolution\n",
        "    final_conv = nn.Conv2d(in_channels=320, out_channels=1280, kernel_size=1, stride=1) # 10*10*320 --> 10*10*1280\n",
        "\n",
        "    \"\"\"\n",
        "      MaxPooling is often preferred for highlighting dominant features, but it can be too aggressive in lightweight models\n",
        "      like MobileNet or EfficientNet, which already have low spatial resolution. It can discard valuable spatial\n",
        "      information. Average Pooling computes the average of nearby pixels, preserving more context. This makes it more\n",
        "      suitable for compact architectures like this one.\n",
        "    \"\"\"\n",
        "\n",
        "    avg_pool = nn.AvgPool2d(10) # 10*10*1280 --> 1*1*1280\n",
        "    flatten = nn.Flatten() # fc layers expect a single tensor not a feature map\n",
        "\n",
        "    # fully connected layers\n",
        "    \"\"\"\n",
        "    The MobileNET paper mapped from 1280 directly to 1000 since it was being trained on ImageNET. My dataset, however, only has 2\n",
        "    output classes: fresh (0) or rotten (1). Mapping directly from 1280 to 2 is an abrupt jump which could limit the model's ability to\n",
        "    distinguish between classes, so I introduced an additional layer to map from 1280 to 500 then applied nonlinearity and\n",
        "    mild dropout before going from 500 to 2.\n",
        "    \"\"\"\n",
        "    fc_1 = nn.Linear(in_features=1280, out_features=500)\n",
        "    fc_2 = nn.Linear(in_features=500, out_features=2)\n",
        "\n",
        "    \"\"\"\n",
        "    The sequence of layers is as follows:\n",
        "\n",
        "      - intial regular convolution\n",
        "      - 4 stacks of DepthwiseSeperableConvolution\n",
        "      - mild dropout for regularization\n",
        "      - 3 more stacks of DepthwiseSeperableConvolution\n",
        "      - 1 final regular convolution\n",
        "      - average pooling\n",
        "      - activation function\n",
        "      - first fully connected linear layer\n",
        "      - activation function\n",
        "      - mild dropout for regularization\n",
        "      - final fully connected linear layer\n",
        "\n",
        "    There are skip connections in blocks 3-6.\n",
        "    \"\"\"\n",
        "\n",
        "    self.layers = nn.Sequential(\n",
        "        block_0,\n",
        "\n",
        "        block_1,\n",
        "        block_2,\n",
        "        block_3,\n",
        "        block_4,\n",
        "        FloatWrapper(dropout_mild),\n",
        "\n",
        "        block_5,\n",
        "        block_6,\n",
        "        block_7,\n",
        "\n",
        "        final_conv,\n",
        "        avg_pool,\n",
        "        flatten,\n",
        "        FloatWrapper(silu),\n",
        "        FloatWrapper(dropout_mild),\n",
        "\n",
        "        fc_1,\n",
        "        FloatWrapper(silu),\n",
        "\n",
        "        fc_2\n",
        "    )\n",
        "\n",
        "  # fusing conv + bn + relu improves speed and efficiency during QAT\n",
        "  def fuse(self):\n",
        "    for module_name, module in self.named_modules(): # iterating over the instances defined in __init__ of this class\n",
        "      if isinstance(module, Block):\n",
        "        fuse_modules(module, [[\"0\", \"1\", \"2\"], [\"4\", \"5\", \"6\"]], inplace=True)\n",
        "      elif module_name == \"layers.0\":\n",
        "        fuse_modules(module, [[\"0\", \"1\", \"2\"]], inplace=True)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.quant(x)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "\n",
        "    x = self.dequant(x)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "GIaS73iFUUBE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "ZCIQf4NUxhgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "966b3fa0-f5c6-4dc0-eaa9-eca21d870b3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-96917922.py:15: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
            "  scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=WARMUP_DUR, eta_min=LR_MIN, max_epochs=EPOCHS)\n"
          ]
        }
      ],
      "source": [
        "cnn = FreshNET()\n",
        "cnn.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss() # std loss function for classification\n",
        "\n",
        "optimizer = optim.Adam(cnn.parameters()) # RMSProp is pretty sensitive to changes in LR so i wanted to try Adam\n",
        "\n",
        "scaler = torch.amp.GradScaler()\n",
        "\n",
        "\"\"\"\n",
        "Not mentioned in the original paper, but this scheduler was used because linear warmup reduces volatility in the earlier epochs.\n",
        "Cosine annealing can improve training stability and convergence and declining LR steadily means the model will rely more on the features\n",
        "it learns early on -- the key differentiators -- rather than picking up potential noise and overfitting.\n",
        "\"\"\"\n",
        "scheduler = LinearWarmupCosineAnnealingLR(optimizer, warmup_epochs=WARMUP_DUR, eta_min=LR_MIN, max_epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and validate"
      ],
      "metadata": {
        "id": "afXvV6nhQynV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(cnn, epochs=EPOCHS, scaler=None):\n",
        "  cnn.train()\n",
        "\n",
        "  for i in range(epochs):\n",
        "    torch.cuda.empty_cache()\n",
        "    correct, total = 0, 0\n",
        "    running_loss = 0\n",
        "\n",
        "    cnn.train(True)\n",
        "\n",
        "    loop = tqdm(train_dataloader, desc=f\"Epoch {i+1}/{epochs}\", leave=True, disable=False)\n",
        "\n",
        "    for j, (input, labels) in enumerate(loop, 1):\n",
        "      input, labels = input.to(device), labels.to(device)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = cnn(input)\n",
        "      loss = loss_fn(output, labels)\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      if scaler:\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "      else:\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      avg_loss = running_loss / j\n",
        "      loop.set_postfix(loss=avg_loss)\n",
        "\n",
        "      pred = torch.argmax(output, dim=1)\n",
        "\n",
        "      total += labels.size(0)\n",
        "      correct += (pred == labels).sum().item()\n",
        "\n",
        "\n",
        "    cnn.eval()\n",
        "    val_total, val_correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for image, label in val_dataloader:\n",
        "        image, label = image.to(device), label.to(device)\n",
        "        output = cnn(image)\n",
        "\n",
        "        pred = torch.argmax(output, dim=1)\n",
        "        val_total += label.size(0)\n",
        "        val_correct += (pred == label).sum().item()\n",
        "\n",
        "      val_accuracy = val_correct / val_total\n",
        "      accuracy = correct / total\n",
        "\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Epoch {i + 1}: LR={current_lr:.6f} \\t Train Acc: {accuracy:.4f} \\t Val Acc: {val_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "pMzwv9TEVZCY"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.to(\"cuda\")\n",
        "compiled_cnn = torch.compile(cnn)\n",
        "\n",
        "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "  train(compiled_cnn, scaler=scaler)"
      ],
      "metadata": {
        "id": "yFiUCT5I6g64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "eb0af289-1c4a-4124-98b1-9269b38f181b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/30: 100%|██████████| 74/74 [01:02<00:00,  1.18it/s, loss=0.695]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: LR=0.000111 \t Train Acc: 0.4526 \t Val Acc: 0.4641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/30:   8%|▊         | 6/74 [00:09<01:45,  1.55s/it, loss=0.695]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1040547688.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompiled_cnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-784405485.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(cnn, epochs, scaler)\u001b[0m\n\u001b[1;32m     21\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m         ), \"No inf checks were recorded for this optimizer.\"\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    349\u001b[0m     ) -> Optional[float]:\n\u001b[1;32m    350\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving model weights"
      ],
      "metadata": {
        "id": "k--Nu6t55uLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.to(\"cpu\")\n",
        "torch.save(cnn.state_dict(), \"/content/modelweight.pth\")\n",
        "\n",
        "from google.colab import files\n",
        "files.download(r\"/content/modelweight.pth\")"
      ],
      "metadata": {
        "id": "Ex61Q4tC5wU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading saved model"
      ],
      "metadata": {
        "id": "iAJdALBLE1Rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "#files.upload()\n",
        "weights = torch.load('/content/modelweight.pth', map_location=torch.device('cpu'))\n",
        "cnn = FreshNET()\n",
        "cnn.load_state_dict(weights)"
      ],
      "metadata": {
        "id": "4tfHgo4CE2fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model size (no quantization)"
      ],
      "metadata": {
        "id": "Gc5_kJ5ebGJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_size = 0\n",
        "\n",
        "for param in cnn.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "\n",
        "buffer_size = 0\n",
        "\n",
        "for buffer in cnn.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('model size: {:.3f}MB'.format(size_all_mb))"
      ],
      "metadata": {
        "id": "Q3Ly9vrv0MVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final test accuracy (no quantization)"
      ],
      "metadata": {
        "id": "WZUAMYaCQ7rT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(test_dataloader, cnn):\n",
        "  with torch.no_grad():\n",
        "    cnn.eval()\n",
        "    accuracy = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in test_dataloader:\n",
        "      images, labels = batch\n",
        "\n",
        "      images = images.to(\"cpu\")\n",
        "      labels = labels.to(\"cpu\")\n",
        "\n",
        "      if images.dtype != torch.float32:\n",
        "        raise Exception(\"Pre-Quantization invalid data type\")\n",
        "\n",
        "      output = cnn(images)\n",
        "\n",
        "      if images.dtype != torch.float32:\n",
        "        raise Exception(\"Post-Quantization invalid data type\")\n",
        "\n",
        "      predicted = torch.argmax(output, dim=1)\n",
        "      accuracy += (predicted == labels).sum().item()\n",
        "      total += labels.size(0)\n",
        "\n",
        "    avg_acc = accuracy / total\n",
        "\n",
        "    return avg_acc\n",
        "\n",
        "\n",
        "cnn.eval()\n",
        "\n",
        "test_acc = test(test_dataloader, cnn)\n",
        "print(f\"Final test accuracy: {100 * test_acc:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Tk4rp3ox4I7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.to(\"cuda\")\n",
        "cnn.eval()\n",
        "\n",
        "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "  test_acc = test(test_dataloader, cnn)\n",
        "  print(f\"Final test accuracy: {100 * test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "uRmnJQDB0LF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantizing model\n",
        "\n",
        "Quantization Aware Training (QAT) involves training the model with full floating point precision, float32, while simulating the effects of int8 quantization. This 'fake' quantization makes the model well-equipped to perform under quantization, maintaining accuracy better than if it used post-training  quantization instead.\n"
      ],
      "metadata": {
        "id": "8oDdlacxT0dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.to(\"cpu\") # model must be on cpu while fusing\n",
        "cnn.eval()\n",
        "cnn.fuse() # custom fuse method fuses the DepthwiseSeperableConvolution layers like conv2d, batchnorm, and relu\n",
        "cnn.to(\"cpu\") # need to switch back to cpu before quantizing\n",
        "config = get_default_qat_qconfig(\"qnnpack\") # for ARM-based architecture, qnnpack is the correct config mode\n",
        "cnn.qconfig = config\n",
        "cnn.train()\n",
        "prepare_qat(cnn, inplace=True) # preparing the model for QAT\n",
        "cnn.eval()\n",
        ""
      ],
      "metadata": {
        "id": "Ad8FNPpRw5wS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b737776-5a92-4a37-fc67-19a58dac6ff5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreshNET(\n",
              "  (quant): QuantStub(\n",
              "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "    )\n",
              "  )\n",
              "  (dequant): DeQuantStub()\n",
              "  (layers): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): ConvReLU2d(\n",
              "        3, 32, kernel_size=(3, 3), stride=(2, 2)\n",
              "        (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "        (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "          fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "          (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "        )\n",
              "      )\n",
              "      (1): Identity()\n",
              "      (2): Identity()\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): DepthwiseSeperableConvolution(\n",
              "        (block): Block(\n",
              "          (0): ConvReLU2d(\n",
              "            32, 32, kernel_size=(1, 1), stride=(1, 1)\n",
              "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "          )\n",
              "          (1): Identity()\n",
              "          (2): Identity()\n",
              "          (3): FloatWrapper(\n",
              "            (layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (4): ConvReLU2d(\n",
              "            32, 16, kernel_size=(1, 1), stride=(1, 1)\n",
              "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "          )\n",
              "          (5): Identity()\n",
              "          (6): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): DepthwiseSeperableConvolution(\n",
              "        (block): Block(\n",
              "          (0): ConvReLU2d(\n",
              "            16, 96, kernel_size=(1, 1), stride=(1, 1)\n",
              "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "          )\n",
              "          (1): Identity()\n",
              "          (2): Identity()\n",
              "          (3): FloatWrapper(\n",
              "            (layer): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (4): ConvReLU2d(\n",
              "            96, 24, kernel_size=(1, 1), stride=(1, 1)\n",
              "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "          )\n",
              "          (5): Identity()\n",
              "          (6): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): DepthwiseSeperableConvolution(\n",
              "        (block): Block(\n",
              "          (0): ConvReLU2d(\n",
              "            24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
              "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "          )\n",
              "          (1): Identity()\n",
              "          (2): Identity()\n",
              "          (3): FloatWrapper(\n",
              "            (layer): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (4): ConvReLU2d(\n",
              "            144, 24, kernel_size=(1, 1), stride=(1, 1)\n",
              "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "          )\n",
              "          (5): Identity()\n",
              "          (6): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): DepthwiseSeperableConvolution(\n",
              "        (block): Block(\n",
              "          (0): ConvReLU2d(\n",
              "            24, 144, kernel_size=(1, 1), stride=(1, 1)\n",
              "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "          )\n",
              "          (1): Identity()\n",
              "          (2): Identity()\n",
              "          (3): FloatWrapper(\n",
              "            (layer): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (4): ConvReLU2d(\n",
              "            144, 32, kernel_size=(1, 1), stride=(1, 1)\n",
              "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "          )\n",
              "          (5): Identity()\n",
              "          (6): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): SkipConnection(\n",
              "        (quant): QuantStub(\n",
              "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "          )\n",
              "        )\n",
              "        (dequant): DeQuantStub()\n",
              "        (seq): Sequential(\n",
              "          (0): DepthwiseSeperableConvolution(\n",
              "            (block): Block(\n",
              "              (0): ConvReLU2d(\n",
              "                32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "                (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "                (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "              )\n",
              "              (1): Identity()\n",
              "              (2): Identity()\n",
              "              (3): FloatWrapper(\n",
              "                (layer): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
              "                (quant): QuantStub()\n",
              "                (dequant): DeQuantStub()\n",
              "              )\n",
              "              (4): ConvReLU2d(\n",
              "                192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
              "                (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "                (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "              )\n",
              "              (5): Identity()\n",
              "              (6): Identity()\n",
              "            )\n",
              "          )\n",
              "          (1): DepthwiseSeperableConvolution(\n",
              "            (block): Block(\n",
              "              (0): ConvReLU2d(\n",
              "                32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "                (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "                (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "              )\n",
              "              (1): Identity()\n",
              "              (2): Identity()\n",
              "              (3): FloatWrapper(\n",
              "                (layer): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
              "                (quant): QuantStub()\n",
              "                (dequant): DeQuantStub()\n",
              "              )\n",
              "              (4): ConvReLU2d(\n",
              "                192, 32, kernel_size=(1, 1), stride=(1, 1)\n",
              "                (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "                (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "              )\n",
              "              (5): Identity()\n",
              "              (6): Identity()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): DepthwiseSeperableConvolution(\n",
              "        (block): Block(\n",
              "          (0): ConvReLU2d(\n",
              "            32, 192, kernel_size=(1, 1), stride=(1, 1)\n",
              "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "          )\n",
              "          (1): Identity()\n",
              "          (2): Identity()\n",
              "          (3): FloatWrapper(\n",
              "            (layer): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (4): ConvReLU2d(\n",
              "            192, 64, kernel_size=(1, 1), stride=(1, 1)\n",
              "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "          )\n",
              "          (5): Identity()\n",
              "          (6): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): SkipConnection(\n",
              "        (quant): QuantStub(\n",
              "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "          )\n",
              "        )\n",
              "        (dequant): DeQuantStub()\n",
              "        (seq): Sequential(\n",
              "          (0): DepthwiseSeperableConvolution(\n",
              "            (block): Block(\n",
              "              (0): ConvReLU2d(\n",
              "                64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
              "                (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "                (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "              )\n",
              "              (1): Identity()\n",
              "              (2): Identity()\n",
              "              (3): FloatWrapper(\n",
              "                (layer): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
              "                (quant): QuantStub()\n",
              "                (dequant): DeQuantStub()\n",
              "              )\n",
              "              (4): ConvReLU2d(\n",
              "                384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
              "                (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "                (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "              )\n",
              "              (5): Identity()\n",
              "              (6): Identity()\n",
              "            )\n",
              "          )\n",
              "          (1): DepthwiseSeperableConvolution(\n",
              "            (block): Block(\n",
              "              (0): ConvReLU2d(\n",
              "                64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
              "                (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "                (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "              )\n",
              "              (1): Identity()\n",
              "              (2): Identity()\n",
              "              (3): FloatWrapper(\n",
              "                (layer): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
              "                (quant): QuantStub()\n",
              "                (dequant): DeQuantStub()\n",
              "              )\n",
              "              (4): ConvReLU2d(\n",
              "                384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
              "                (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "                (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "              )\n",
              "              (5): Identity()\n",
              "              (6): Identity()\n",
              "            )\n",
              "          )\n",
              "          (2): DepthwiseSeperableConvolution(\n",
              "            (block): Block(\n",
              "              (0): ConvReLU2d(\n",
              "                64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
              "                (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "                (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "              )\n",
              "              (1): Identity()\n",
              "              (2): Identity()\n",
              "              (3): FloatWrapper(\n",
              "                (layer): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
              "                (quant): QuantStub()\n",
              "                (dequant): DeQuantStub()\n",
              "              )\n",
              "              (4): ConvReLU2d(\n",
              "                384, 64, kernel_size=(1, 1), stride=(1, 1)\n",
              "                (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "                (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "              )\n",
              "              (5): Identity()\n",
              "              (6): Identity()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): FloatWrapper(\n",
              "      (layer): Dropout(p=0.1, inplace=False)\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): DepthwiseSeperableConvolution(\n",
              "        (block): Block(\n",
              "          (0): ConvReLU2d(\n",
              "            64, 384, kernel_size=(1, 1), stride=(1, 1)\n",
              "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "          )\n",
              "          (1): Identity()\n",
              "          (2): Identity()\n",
              "          (3): FloatWrapper(\n",
              "            (layer): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (4): ConvReLU2d(\n",
              "            384, 96, kernel_size=(1, 1), stride=(1, 1)\n",
              "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "          )\n",
              "          (5): Identity()\n",
              "          (6): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): SkipConnection(\n",
              "        (quant): QuantStub(\n",
              "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "          )\n",
              "        )\n",
              "        (dequant): DeQuantStub()\n",
              "        (seq): Sequential(\n",
              "          (0): DepthwiseSeperableConvolution(\n",
              "            (block): Block(\n",
              "              (0): ConvReLU2d(\n",
              "                96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
              "                (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "                (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "              )\n",
              "              (1): Identity()\n",
              "              (2): Identity()\n",
              "              (3): FloatWrapper(\n",
              "                (layer): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)\n",
              "                (quant): QuantStub()\n",
              "                (dequant): DeQuantStub()\n",
              "              )\n",
              "              (4): ConvReLU2d(\n",
              "                576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
              "                (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "                (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "              )\n",
              "              (5): Identity()\n",
              "              (6): Identity()\n",
              "            )\n",
              "          )\n",
              "          (1): DepthwiseSeperableConvolution(\n",
              "            (block): Block(\n",
              "              (0): ConvReLU2d(\n",
              "                96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
              "                (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "                (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "              )\n",
              "              (1): Identity()\n",
              "              (2): Identity()\n",
              "              (3): FloatWrapper(\n",
              "                (layer): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)\n",
              "                (quant): QuantStub()\n",
              "                (dequant): DeQuantStub()\n",
              "              )\n",
              "              (4): ConvReLU2d(\n",
              "                576, 96, kernel_size=(1, 1), stride=(1, 1)\n",
              "                (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "                (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "              )\n",
              "              (5): Identity()\n",
              "              (6): Identity()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): DepthwiseSeperableConvolution(\n",
              "        (block): Block(\n",
              "          (0): ConvReLU2d(\n",
              "            96, 576, kernel_size=(1, 1), stride=(1, 1)\n",
              "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "          )\n",
              "          (1): Identity()\n",
              "          (2): Identity()\n",
              "          (3): FloatWrapper(\n",
              "            (layer): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (4): ConvReLU2d(\n",
              "            576, 160, kernel_size=(1, 1), stride=(1, 1)\n",
              "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "          )\n",
              "          (5): Identity()\n",
              "          (6): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): SkipConnection(\n",
              "        (quant): QuantStub(\n",
              "          (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "            fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "            (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "          )\n",
              "        )\n",
              "        (dequant): DeQuantStub()\n",
              "        (seq): Sequential(\n",
              "          (0): DepthwiseSeperableConvolution(\n",
              "            (block): Block(\n",
              "              (0): ConvReLU2d(\n",
              "                160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "                (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "                (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "              )\n",
              "              (1): Identity()\n",
              "              (2): Identity()\n",
              "              (3): FloatWrapper(\n",
              "                (layer): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
              "                (quant): QuantStub()\n",
              "                (dequant): DeQuantStub()\n",
              "              )\n",
              "              (4): ConvReLU2d(\n",
              "                960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
              "                (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "                (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "              )\n",
              "              (5): Identity()\n",
              "              (6): Identity()\n",
              "            )\n",
              "          )\n",
              "          (1): DepthwiseSeperableConvolution(\n",
              "            (block): Block(\n",
              "              (0): ConvReLU2d(\n",
              "                160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "                (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "                (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "              )\n",
              "              (1): Identity()\n",
              "              (2): Identity()\n",
              "              (3): FloatWrapper(\n",
              "                (layer): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
              "                (quant): QuantStub()\n",
              "                (dequant): DeQuantStub()\n",
              "              )\n",
              "              (4): ConvReLU2d(\n",
              "                960, 160, kernel_size=(1, 1), stride=(1, 1)\n",
              "                (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "                (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "                  fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "                  (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "                )\n",
              "              )\n",
              "              (5): Identity()\n",
              "              (6): Identity()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8): Sequential(\n",
              "      (0): DepthwiseSeperableConvolution(\n",
              "        (block): Block(\n",
              "          (0): ConvReLU2d(\n",
              "            160, 960, kernel_size=(1, 1), stride=(1, 1)\n",
              "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "          )\n",
              "          (1): Identity()\n",
              "          (2): Identity()\n",
              "          (3): FloatWrapper(\n",
              "            (layer): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (4): ConvReLU2d(\n",
              "            960, 320, kernel_size=(1, 1), stride=(1, 1)\n",
              "            (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "            (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "              fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "              (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "            )\n",
              "          )\n",
              "          (5): Identity()\n",
              "          (6): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): Conv2d(\n",
              "      320, 1280, kernel_size=(1, 1), stride=(1, 1)\n",
              "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "      )\n",
              "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "      )\n",
              "    )\n",
              "    (10): AvgPool2d(kernel_size=10, stride=10, padding=0)\n",
              "    (11): Flatten(start_dim=1, end_dim=-1)\n",
              "    (12): FloatWrapper(\n",
              "      (layer): SiLU()\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "    )\n",
              "    (13): FloatWrapper(\n",
              "      (layer): Dropout(p=0.1, inplace=False)\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "    )\n",
              "    (14): Linear(\n",
              "      in_features=1280, out_features=500, bias=True\n",
              "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "      )\n",
              "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "      )\n",
              "    )\n",
              "    (15): FloatWrapper(\n",
              "      (layer): SiLU()\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "    )\n",
              "    (16): Linear(\n",
              "      in_features=500, out_features=2, bias=True\n",
              "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
              "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_tensor_symmetric, reduce_range=False\n",
              "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "      )\n",
              "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
              "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine, reduce_range=False\n",
              "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calibrating model before converting to get meaningful zero_point and scale values\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in calibrate_dataloader:\n",
        "        images, _ = batch\n",
        "        images = images.to(\"cpu\")\n",
        "        output = cnn(images)\n",
        "\n",
        "\n",
        "for name, module in cnn.named_modules():\n",
        "    if hasattr(module, \"activation_post_process\"):\n",
        "        obs = module.activation_post_process\n",
        "        if hasattr(obs, \"calculate_qparams\"):\n",
        "            scale, zero_point = obs.calculate_qparams()\n",
        "            print(f\"Using observer from: {name}\")\n",
        "            break\n",
        "\n",
        "scale = scale.item()\n",
        "zero_point = zero_point.item()\n",
        "\n",
        "print(scale)\n",
        "print(zero_point)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcME8HuDw9iN",
        "outputId": "f62a9106-759f-48e6-d0e6-7c64af8ba87e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using observer from: quant\n",
            "0.01688021793961525\n",
            "99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_cnn = convert(cnn, inplace=False)\n",
        "quantized_cnn.to(\"cpu\")"
      ],
      "metadata": {
        "id": "ng4a2FNZSP7u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f935a6b8-a6f3-42db-fce1-a4b955d23cb3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreshNET(\n",
              "  (quant): Quantize(scale=tensor([0.0169]), zero_point=tensor([99]), dtype=torch.quint8)\n",
              "  (dequant): DeQuantize()\n",
              "  (layers): Sequential(\n",
              "    (0): Sequential(\n",
              "      (0): QuantizedConvReLU2d(3, 32, kernel_size=(3, 3), stride=(2, 2), scale=0.0204971544444561, zero_point=0)\n",
              "      (1): Identity()\n",
              "      (2): Identity()\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): DepthwiseSeperableConvolution(\n",
              "        (block): Block(\n",
              "          (0): QuantizedConvReLU2d(32, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.02337159588932991, zero_point=0)\n",
              "          (1): Identity()\n",
              "          (2): Identity()\n",
              "          (3): FloatWrapper(\n",
              "            (layer): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (4): QuantizedConvReLU2d(32, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.023689599707722664, zero_point=0)\n",
              "          (5): Identity()\n",
              "          (6): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): DepthwiseSeperableConvolution(\n",
              "        (block): Block(\n",
              "          (0): QuantizedConvReLU2d(16, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.03355472907423973, zero_point=0)\n",
              "          (1): Identity()\n",
              "          (2): Identity()\n",
              "          (3): FloatWrapper(\n",
              "            (layer): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (4): QuantizedConvReLU2d(96, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.02727613039314747, zero_point=0)\n",
              "          (5): Identity()\n",
              "          (6): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): DepthwiseSeperableConvolution(\n",
              "        (block): Block(\n",
              "          (0): QuantizedConvReLU2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.026626503095030785, zero_point=0)\n",
              "          (1): Identity()\n",
              "          (2): Identity()\n",
              "          (3): FloatWrapper(\n",
              "            (layer): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (4): QuantizedConvReLU2d(144, 24, kernel_size=(1, 1), stride=(1, 1), scale=0.02050282619893551, zero_point=0)\n",
              "          (5): Identity()\n",
              "          (6): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): DepthwiseSeperableConvolution(\n",
              "        (block): Block(\n",
              "          (0): QuantizedConvReLU2d(24, 144, kernel_size=(1, 1), stride=(1, 1), scale=0.02163819782435894, zero_point=0)\n",
              "          (1): Identity()\n",
              "          (2): Identity()\n",
              "          (3): FloatWrapper(\n",
              "            (layer): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (4): QuantizedConvReLU2d(144, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.015290580689907074, zero_point=0)\n",
              "          (5): Identity()\n",
              "          (6): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): SkipConnection(\n",
              "        (quant): Quantize(scale=tensor([0.0207]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "        (dequant): DeQuantize()\n",
              "        (seq): Sequential(\n",
              "          (0): DepthwiseSeperableConvolution(\n",
              "            (block): Block(\n",
              "              (0): QuantizedConvReLU2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.018854746595025063, zero_point=0)\n",
              "              (1): Identity()\n",
              "              (2): Identity()\n",
              "              (3): FloatWrapper(\n",
              "                (layer): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
              "                (quant): QuantStub()\n",
              "                (dequant): DeQuantStub()\n",
              "              )\n",
              "              (4): QuantizedConvReLU2d(192, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.013271795585751534, zero_point=0)\n",
              "              (5): Identity()\n",
              "              (6): Identity()\n",
              "            )\n",
              "          )\n",
              "          (1): DepthwiseSeperableConvolution(\n",
              "            (block): Block(\n",
              "              (0): QuantizedConvReLU2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.015161764807999134, zero_point=0)\n",
              "              (1): Identity()\n",
              "              (2): Identity()\n",
              "              (3): FloatWrapper(\n",
              "                (layer): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
              "                (quant): QuantStub()\n",
              "                (dequant): DeQuantStub()\n",
              "              )\n",
              "              (4): QuantizedConvReLU2d(192, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.012229593470692635, zero_point=0)\n",
              "              (5): Identity()\n",
              "              (6): Identity()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): DepthwiseSeperableConvolution(\n",
              "        (block): Block(\n",
              "          (0): QuantizedConvReLU2d(32, 192, kernel_size=(1, 1), stride=(1, 1), scale=0.01918076165020466, zero_point=0)\n",
              "          (1): Identity()\n",
              "          (2): Identity()\n",
              "          (3): FloatWrapper(\n",
              "            (layer): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (4): QuantizedConvReLU2d(192, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.011944214813411236, zero_point=0)\n",
              "          (5): Identity()\n",
              "          (6): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): SkipConnection(\n",
              "        (quant): Quantize(scale=tensor([0.0146]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "        (dequant): DeQuantize()\n",
              "        (seq): Sequential(\n",
              "          (0): DepthwiseSeperableConvolution(\n",
              "            (block): Block(\n",
              "              (0): QuantizedConvReLU2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.016087127849459648, zero_point=0)\n",
              "              (1): Identity()\n",
              "              (2): Identity()\n",
              "              (3): FloatWrapper(\n",
              "                (layer): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
              "                (quant): QuantStub()\n",
              "                (dequant): DeQuantStub()\n",
              "              )\n",
              "              (4): QuantizedConvReLU2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.009958397597074509, zero_point=0)\n",
              "              (5): Identity()\n",
              "              (6): Identity()\n",
              "            )\n",
              "          )\n",
              "          (1): DepthwiseSeperableConvolution(\n",
              "            (block): Block(\n",
              "              (0): QuantizedConvReLU2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.011901065707206726, zero_point=0)\n",
              "              (1): Identity()\n",
              "              (2): Identity()\n",
              "              (3): FloatWrapper(\n",
              "                (layer): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
              "                (quant): QuantStub()\n",
              "                (dequant): DeQuantStub()\n",
              "              )\n",
              "              (4): QuantizedConvReLU2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.00931444764137268, zero_point=0)\n",
              "              (5): Identity()\n",
              "              (6): Identity()\n",
              "            )\n",
              "          )\n",
              "          (2): DepthwiseSeperableConvolution(\n",
              "            (block): Block(\n",
              "              (0): QuantizedConvReLU2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.010696567595005035, zero_point=0)\n",
              "              (1): Identity()\n",
              "              (2): Identity()\n",
              "              (3): FloatWrapper(\n",
              "                (layer): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
              "                (quant): QuantStub()\n",
              "                (dequant): DeQuantStub()\n",
              "              )\n",
              "              (4): QuantizedConvReLU2d(384, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.010319046676158905, zero_point=0)\n",
              "              (5): Identity()\n",
              "              (6): Identity()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): FloatWrapper(\n",
              "      (layer): Dropout(p=0.1, inplace=False)\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): DepthwiseSeperableConvolution(\n",
              "        (block): Block(\n",
              "          (0): QuantizedConvReLU2d(64, 384, kernel_size=(1, 1), stride=(1, 1), scale=0.012154867872595787, zero_point=0)\n",
              "          (1): Identity()\n",
              "          (2): Identity()\n",
              "          (3): FloatWrapper(\n",
              "            (layer): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (4): QuantizedConvReLU2d(384, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.009631888940930367, zero_point=0)\n",
              "          (5): Identity()\n",
              "          (6): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): SkipConnection(\n",
              "        (quant): Quantize(scale=tensor([0.0131]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "        (dequant): DeQuantize()\n",
              "        (seq): Sequential(\n",
              "          (0): DepthwiseSeperableConvolution(\n",
              "            (block): Block(\n",
              "              (0): QuantizedConvReLU2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.011499051004648209, zero_point=0)\n",
              "              (1): Identity()\n",
              "              (2): Identity()\n",
              "              (3): FloatWrapper(\n",
              "                (layer): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)\n",
              "                (quant): QuantStub()\n",
              "                (dequant): DeQuantStub()\n",
              "              )\n",
              "              (4): QuantizedConvReLU2d(576, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.009455030784010887, zero_point=0)\n",
              "              (5): Identity()\n",
              "              (6): Identity()\n",
              "            )\n",
              "          )\n",
              "          (1): DepthwiseSeperableConvolution(\n",
              "            (block): Block(\n",
              "              (0): QuantizedConvReLU2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.010075717233121395, zero_point=0)\n",
              "              (1): Identity()\n",
              "              (2): Identity()\n",
              "              (3): FloatWrapper(\n",
              "                (layer): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576)\n",
              "                (quant): QuantStub()\n",
              "                (dequant): DeQuantStub()\n",
              "              )\n",
              "              (4): QuantizedConvReLU2d(576, 96, kernel_size=(1, 1), stride=(1, 1), scale=0.008806249126791954, zero_point=0)\n",
              "              (5): Identity()\n",
              "              (6): Identity()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): DepthwiseSeperableConvolution(\n",
              "        (block): Block(\n",
              "          (0): QuantizedConvReLU2d(96, 576, kernel_size=(1, 1), stride=(1, 1), scale=0.010197245515882969, zero_point=0)\n",
              "          (1): Identity()\n",
              "          (2): Identity()\n",
              "          (3): FloatWrapper(\n",
              "            (layer): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (4): QuantizedConvReLU2d(576, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.010025879368185997, zero_point=0)\n",
              "          (5): Identity()\n",
              "          (6): Identity()\n",
              "        )\n",
              "      )\n",
              "      (1): SkipConnection(\n",
              "        (quant): Quantize(scale=tensor([0.0124]), zero_point=tensor([0]), dtype=torch.quint8)\n",
              "        (dequant): DeQuantize()\n",
              "        (seq): Sequential(\n",
              "          (0): DepthwiseSeperableConvolution(\n",
              "            (block): Block(\n",
              "              (0): QuantizedConvReLU2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.009997330605983734, zero_point=0)\n",
              "              (1): Identity()\n",
              "              (2): Identity()\n",
              "              (3): FloatWrapper(\n",
              "                (layer): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
              "                (quant): QuantStub()\n",
              "                (dequant): DeQuantStub()\n",
              "              )\n",
              "              (4): QuantizedConvReLU2d(960, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.008836109191179276, zero_point=0)\n",
              "              (5): Identity()\n",
              "              (6): Identity()\n",
              "            )\n",
              "          )\n",
              "          (1): DepthwiseSeperableConvolution(\n",
              "            (block): Block(\n",
              "              (0): QuantizedConvReLU2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.009785894304513931, zero_point=0)\n",
              "              (1): Identity()\n",
              "              (2): Identity()\n",
              "              (3): FloatWrapper(\n",
              "                (layer): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
              "                (quant): QuantStub()\n",
              "                (dequant): DeQuantStub()\n",
              "              )\n",
              "              (4): QuantizedConvReLU2d(960, 160, kernel_size=(1, 1), stride=(1, 1), scale=0.00962571706622839, zero_point=0)\n",
              "              (5): Identity()\n",
              "              (6): Identity()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8): Sequential(\n",
              "      (0): DepthwiseSeperableConvolution(\n",
              "        (block): Block(\n",
              "          (0): QuantizedConvReLU2d(160, 960, kernel_size=(1, 1), stride=(1, 1), scale=0.01049036718904972, zero_point=0)\n",
              "          (1): Identity()\n",
              "          (2): Identity()\n",
              "          (3): FloatWrapper(\n",
              "            (layer): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960)\n",
              "            (quant): QuantStub()\n",
              "            (dequant): DeQuantStub()\n",
              "          )\n",
              "          (4): QuantizedConvReLU2d(960, 320, kernel_size=(1, 1), stride=(1, 1), scale=0.010215499438345432, zero_point=0)\n",
              "          (5): Identity()\n",
              "          (6): Identity()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): QuantizedConv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), scale=0.008440681733191013, zero_point=129)\n",
              "    (10): AvgPool2d(kernel_size=10, stride=10, padding=0)\n",
              "    (11): Flatten(start_dim=1, end_dim=-1)\n",
              "    (12): FloatWrapper(\n",
              "      (layer): SiLU()\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "    )\n",
              "    (13): FloatWrapper(\n",
              "      (layer): Dropout(p=0.1, inplace=False)\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "    )\n",
              "    (14): QuantizedLinear(in_features=1280, out_features=500, scale=0.0014463691040873528, zero_point=111, qscheme=torch.per_tensor_affine)\n",
              "    (15): FloatWrapper(\n",
              "      (layer): SiLU()\n",
              "      (quant): QuantStub()\n",
              "      (dequant): DeQuantStub()\n",
              "    )\n",
              "    (16): QuantizedLinear(in_features=500, out_features=2, scale=0.00014677809667773545, zero_point=0, qscheme=torch.per_tensor_affine)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in quantized_cnn.named_modules():\n",
        "    if isinstance(module, torch.nn.quantized.Conv2d):\n",
        "      if module.bias() is not None:\n",
        "        dataType = module.bias().dtype\n",
        "        print(f\"{name} bias dtype: {dataType}\")"
      ],
      "metadata": {
        "id": "NZkgE72dTNTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9080e283-0b01-47a2-c062-5a9008dff5c8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0 bias dtype: torch.float32\n",
            "layers.1.0.block.0 bias dtype: torch.float32\n",
            "layers.1.0.block.4 bias dtype: torch.float32\n",
            "layers.2.0.block.0 bias dtype: torch.float32\n",
            "layers.2.0.block.4 bias dtype: torch.float32\n",
            "layers.2.1.block.0 bias dtype: torch.float32\n",
            "layers.2.1.block.4 bias dtype: torch.float32\n",
            "layers.3.0.block.0 bias dtype: torch.float32\n",
            "layers.3.0.block.4 bias dtype: torch.float32\n",
            "layers.3.1.seq.0.block.0 bias dtype: torch.float32\n",
            "layers.3.1.seq.0.block.4 bias dtype: torch.float32\n",
            "layers.3.1.seq.1.block.0 bias dtype: torch.float32\n",
            "layers.3.1.seq.1.block.4 bias dtype: torch.float32\n",
            "layers.4.0.block.0 bias dtype: torch.float32\n",
            "layers.4.0.block.4 bias dtype: torch.float32\n",
            "layers.4.1.seq.0.block.0 bias dtype: torch.float32\n",
            "layers.4.1.seq.0.block.4 bias dtype: torch.float32\n",
            "layers.4.1.seq.1.block.0 bias dtype: torch.float32\n",
            "layers.4.1.seq.1.block.4 bias dtype: torch.float32\n",
            "layers.4.1.seq.2.block.0 bias dtype: torch.float32\n",
            "layers.4.1.seq.2.block.4 bias dtype: torch.float32\n",
            "layers.6.0.block.0 bias dtype: torch.float32\n",
            "layers.6.0.block.4 bias dtype: torch.float32\n",
            "layers.6.1.seq.0.block.0 bias dtype: torch.float32\n",
            "layers.6.1.seq.0.block.4 bias dtype: torch.float32\n",
            "layers.6.1.seq.1.block.0 bias dtype: torch.float32\n",
            "layers.6.1.seq.1.block.4 bias dtype: torch.float32\n",
            "layers.7.0.block.0 bias dtype: torch.float32\n",
            "layers.7.0.block.4 bias dtype: torch.float32\n",
            "layers.7.1.seq.0.block.0 bias dtype: torch.float32\n",
            "layers.7.1.seq.0.block.4 bias dtype: torch.float32\n",
            "layers.7.1.seq.1.block.0 bias dtype: torch.float32\n",
            "layers.7.1.seq.1.block.4 bias dtype: torch.float32\n",
            "layers.8.0.block.0 bias dtype: torch.float32\n",
            "layers.8.0.block.4 bias dtype: torch.float32\n",
            "layers.9 bias dtype: torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model size (with quantization)\n"
      ],
      "metadata": {
        "id": "1A0yeJyQbL6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_size = 0\n",
        "\n",
        "for param in quantized_cnn.parameters():\n",
        "    param_size += param.nelement() * param.element_size()\n",
        "\n",
        "buffer_size = 0\n",
        "\n",
        "for buffer in quantized_cnn.buffers():\n",
        "    buffer_size += buffer.nelement() * buffer.element_size()\n",
        "\n",
        "size_all_mb = (param_size + buffer_size) / 1024**2\n",
        "print('model size: {:.3f}MB'.format(size_all_mb))"
      ],
      "metadata": {
        "id": "fQOpdSI-0Jrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final test accuracy (with quantization)"
      ],
      "metadata": {
        "id": "4eaqc1MPan3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_cnn.eval()\n",
        "quantized_cnn.to(\"cuda\")\n",
        "test_acc = test(test_dataloader, quantized_cnn)\n",
        "print(f\"Final test accuracy of quantized model: {100 * test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "eXnvHqmsnBmB"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyO0qPTvC9SahxdFJt71gp9c",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}